{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "M = 32\n",
    "\n",
    "\n",
    "def flashattn(Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor) -> torch.Tensor:\n",
    "    assert Q.shape == K.shape == V.shape\n",
    "    assert len(Q.shape) == 2\n",
    "\n",
    "    K = K.T\n",
    "\n",
    "    # seq length, inner dim of representations\n",
    "    N, d = Q.shape\n",
    "\n",
    "    # of kv vectors per tile\n",
    "    bc = math.ceil(M/(4*d))\n",
    "    tc = math.ceil(N/bc)\n",
    "    K_shared = torch.empty((d, bc), dtype=Q.dtype)\n",
    "    V_shared = torch.empty((bc, d), dtype=Q.dtype)\n",
    "\n",
    "    # of q vectors per tile\n",
    "    br = min(math.ceil(M/(4*d)), d)\n",
    "    tr = math.ceil(N/br)\n",
    "    Q_shared = torch.empty((br, d), dtype=Q.dtype)\n",
    "\n",
    "    # print(f'bc={bc}, br={br}')\n",
    "    # print(f'tc={tc}, tr={tr}')\n",
    "\n",
    "    # output matrix\n",
    "    O = torch.zeros_like(Q)\n",
    "    O_shared = torch.empty_like(Q_shared)\n",
    "\n",
    "    # intermediate rowmaxes\n",
    "    m = torch.full((N,), -torch.inf, dtype=Q.dtype)\n",
    "    m_shared = torch.empty((br, 1), dtype=Q.dtype)\n",
    "    # intermediate normalization constants\n",
    "    l = torch.full((N,), 0, dtype=Q.dtype)\n",
    "    l_shared = torch.empty((br, 1), dtype=Q.dtype)\n",
    "\n",
    "    for i in range(tc):\n",
    "        # load k, v chunks\n",
    "        # make sure we load in k as its transposed version\n",
    "        K_shared[:, :] = K[:, i*bc:(i+1)*bc]\n",
    "        V_shared[:, :] = V[i*bc:(i+1)*bc, :]\n",
    "\n",
    "        for j in range(tr):\n",
    "            # load in q, o, m, l\n",
    "            Q_shared[:, :] = Q[j*br:(j+1)*br, :]\n",
    "            # if i == 0: print(f'Q: {Q_shared}')\n",
    "            O_shared[:, :] = O[j*br:(j+1)*br, :]\n",
    "            m_shared[:, :] = m[j*br:(j+1)*br].unsqueeze(-1)\n",
    "            l_shared[:, :] = l[j*br:(j+1)*br].unsqueeze(-1)\n",
    "            \n",
    "            S = Q_shared @ K_shared\n",
    "\n",
    "            # get row-wise softmax statistics\n",
    "            mt = S.max(dim=1).values.reshape(-1, 1)\n",
    "\n",
    "            Pt = torch.exp(S - mt)\n",
    "            lt = Pt.sum(dim=1).reshape(-1, 1)\n",
    "\n",
    "            # compute new statistics\n",
    "            m_new = torch.max(mt, m_shared)\n",
    "            l_new = (torch.exp(m_shared - m_new) * l_shared) + (torch.exp(mt - m_new) * lt)\n",
    "\n",
    "\n",
    "            # update chunk of output\n",
    "            O_new = (l_shared * torch.exp(m_shared - m_new) * O_shared + torch.exp(mt - m_new) * Pt @ V_shared) / l_new\n",
    "            O[j*br:(j+1)*br, :] = O_new\n",
    "            \n",
    "            m[j*br:(j+1)*br] = m_new.flatten()\n",
    "            l[j*br:(j+1)*br] = l_new.flatten()\n",
    "\n",
    "    return O\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.5405, 0.8691],\n",
       "         [0.8764, 0.3651],\n",
       "         [0.6020, 0.1577],\n",
       "         [0.3892, 0.7547],\n",
       "         [0.8394, 0.7066],\n",
       "         [0.1834, 0.8229],\n",
       "         [0.3026, 0.8099],\n",
       "         [0.9580, 0.4797]], device='cuda:0'),\n",
       " tensor([[0.7769, 0.8157],\n",
       "         [0.9311, 0.5300],\n",
       "         [0.3401, 0.5987],\n",
       "         [0.0919, 0.7284],\n",
       "         [0.5994, 0.8746],\n",
       "         [0.9553, 0.6966],\n",
       "         [0.1698, 0.8951],\n",
       "         [0.5294, 0.5202]], device='cuda:0'),\n",
       " tensor([[0.8813, 0.4181],\n",
       "         [0.0740, 0.2983],\n",
       "         [0.1123, 0.9229],\n",
       "         [0.8420, 0.2798],\n",
       "         [0.1049, 0.9854],\n",
       "         [0.4296, 0.1808],\n",
       "         [0.9028, 0.7446],\n",
       "         [0.5205, 0.8607]], device='cuda:0'))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dumb_attn(Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"equivalent to F.scaled_dot_product_attention(Q, K, V, scale=1)\"\"\"\n",
    "    # P = (Q @ K.T)\n",
    "    # m = P.max(dim=1).values.reshape(-1, 1)\n",
    "    # print((P - m).exp())\n",
    "    # return torch.softmax(Q @ K.T, dim=1) @ V\n",
    "    return F.scaled_dot_product_attention(Q, K, V, scale=1)\n",
    "\n",
    "\n",
    "Q = torch.rand((8, 2), dtype=torch.float32, device='cuda')\n",
    "K = torch.rand((8, 2), dtype=torch.float32, device='cuda')\n",
    "V = torch.rand((8, 2), dtype=torch.float32, device='cuda')\n",
    "\n",
    "# Q = torch.randint(1, 9, (2, 2), device='cuda').float()\n",
    "# K = torch.randint(1, 9, (2, 2), device='cuda').float()\n",
    "# V = torch.randint(1, 9, (2, 2), device='cuda').float()\n",
    "\n",
    "# Q = torch.tensor([[0., 1.],\n",
    "#          [2., 3.]])\n",
    "\n",
    "# K = torch.tensor([[0., 1.5],\n",
    "#          [2., 3.]])\n",
    "\n",
    "# V = torch.tensor([[1., 0.],\n",
    "#          [0., 1.]])\n",
    "Q, K, V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/seb/.cache/torch_extensions/py312_cu121 as PyTorch extensions root...\n",
      "The input conditions for extension module m have changed. Bumping to version 14 and re-building as m_v14...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/seb/.cache/torch_extensions/py312_cu121/m/build.ninja...\n",
      "/home/seb/CUDA/cudaenv/lib/python3.12/site-packages/torch/utils/cpp_extension.py:1967: UserWarning: TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. \n",
      "If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'].\n",
      "  warnings.warn(\n",
      "Building extension module m_v14...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/3] c++ -MMD -MF main.o.d -DTORCH_EXTENSION_NAME=m_v14 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/seb/CUDA/cudaenv/lib/python3.12/site-packages/torch/include -isystem /home/seb/CUDA/cudaenv/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -isystem /home/seb/CUDA/cudaenv/lib/python3.12/site-packages/torch/include/TH -isystem /home/seb/CUDA/cudaenv/lib/python3.12/site-packages/torch/include/THC -isystem /usr/local/cuda-12.3/include -isystem /home/seb/miniconda3/include/python3.12 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -c /home/seb/CUDA/flash-attention/main.cpp -o main.o \n",
      "[2/3] /usr/local/cuda-12.3/bin/nvcc --generate-dependencies-with-compile --dependency-output flash_attention.cuda.o.d -DTORCH_EXTENSION_NAME=m_v14 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/seb/CUDA/cudaenv/lib/python3.12/site-packages/torch/include -isystem /home/seb/CUDA/cudaenv/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -isystem /home/seb/CUDA/cudaenv/lib/python3.12/site-packages/torch/include/TH -isystem /home/seb/CUDA/cudaenv/lib/python3.12/site-packages/torch/include/THC -isystem /usr/local/cuda-12.3/include -isystem /home/seb/miniconda3/include/python3.12 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_61,code=compute_61 -gencode=arch=compute_61,code=sm_61 --compiler-options '-fPIC' -std=c++17 -c /home/seb/CUDA/flash-attention/flash_attention.cu -o flash_attention.cuda.o \n",
      "[3/3] c++ main.o flash_attention.cuda.o -shared -L/home/seb/CUDA/cudaenv/lib/python3.12/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda-12.3/lib64 -lcudart -o m_v14.so\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module m_v14...\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.cpp_extension import load\n",
    "module = load(\n",
    "    name='m',\n",
    "    sources=['main.cpp', 'flash_attention.cu'],\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n",
      "O_s should be zeros on first iter, than something else (before being multiplied by 0.000000):\n",
      "[[0.0000, 0.0000, ]\n",
      "[0.0000, 0.0000, ]]\n",
      "\n",
      "[[0.0000, 0.0000, ]\n",
      "[0.0000, 0.0000, ]]\n",
      "\n",
      "O_s should be zeros on first iter, than something else (before being multiplied by 0.735759):\n",
      "[[0.0000, 0.0000, ]\n",
      "[0.0000, 0.0000, ]]\n",
      "\n",
      "[[0.0000, 0.0000, ]\n",
      "[0.0000, 0.0000, ]]\n",
      "\n",
      "O_s should be zeros on first iter, than something else (before being multiplied by 2.735759):\n",
      "[[0.0000, 0.0000, ]\n",
      "[0.0000, 0.0000, ]]\n",
      "\n",
      "[[0.0000, 0.0000, ]\n",
      "[0.0000, 0.0000, ]]\n",
      "\n",
      "O_s should be zeros on first iter, than something else (before being multiplied by 4.735759):\n",
      "[[0.0000, 0.0000, ]\n",
      "[0.0000, 0.0000, ]]\n",
      "\n",
      "[[0.0000, 0.0000, ]\n",
      "[0.0000, 0.0000, ]]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0.4790, 0.5707],\n",
       "         [0.4560, 0.5515],\n",
       "         [0.4619, 0.5627],\n",
       "         [0.4827, 0.5761],\n",
       "         [0.4644, 0.5560],\n",
       "         [0.4928, 0.5846],\n",
       "         [0.4875, 0.5800],\n",
       "         [0.4555, 0.5485]], device='cuda:0'),\n",
       " tensor([[0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.]], device='cuda:0'))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.isclose(dumb_attn(Q, K, V), flashattn(Q, K, V))\n",
    "# dumb_attn(Q, K, V), flashattn(Q, K, V)\n",
    "dumb_attn(Q, K, V), module.flash_attn(Q, K, V)\n",
    "# F.scaled_dot_product_attention(Q, K, V, scale=1), flashattn(Q, K, V)\n",
    "# module.flash_attn(Q, K, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cudaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
