{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure to use the right version of pytorch when testing on nvidia/amd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "torch.set_printoptions(sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "M = 32\n",
    "\n",
    "\n",
    "def flashattn(Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor) -> torch.Tensor:\n",
    "    assert Q.shape == K.shape == V.shape\n",
    "    assert len(Q.shape) == 2\n",
    "\n",
    "    K = K.T\n",
    "\n",
    "    # seq length, inner dim of representations\n",
    "    N, d = Q.shape\n",
    "\n",
    "    # of kv vectors per tile\n",
    "    bc = math.ceil(M/(4*d))\n",
    "    tc = math.ceil(N/bc)\n",
    "    K_shared = torch.empty((d, bc), dtype=Q.dtype)\n",
    "    V_shared = torch.empty((bc, d), dtype=Q.dtype)\n",
    "\n",
    "    # of q vectors per tile\n",
    "    br = min(math.ceil(M/(4*d)), d)\n",
    "    tr = math.ceil(N/br)\n",
    "    Q_shared = torch.empty((br, d), dtype=Q.dtype)\n",
    "\n",
    "    # print(f'bc={bc}, br={br}')\n",
    "    # print(f'tc={tc}, tr={tr}')\n",
    "\n",
    "    # output matrix\n",
    "    O = torch.zeros_like(Q)\n",
    "    O_shared = torch.empty_like(Q_shared)\n",
    "\n",
    "    # intermediate rowmaxes\n",
    "    m = torch.full((N,), -torch.inf, dtype=Q.dtype)\n",
    "    m_shared = torch.empty((br, 1), dtype=Q.dtype)\n",
    "    # intermediate normalization constants\n",
    "    l = torch.full((N,), 0, dtype=Q.dtype)\n",
    "    l_shared = torch.empty((br, 1), dtype=Q.dtype)\n",
    "\n",
    "    for i in range(tc):\n",
    "        # load k, v chunks\n",
    "        # make sure we load in k as its transposed version\n",
    "        K_shared[:, :] = K[:, i*bc:(i+1)*bc]\n",
    "        V_shared[:, :] = V[i*bc:(i+1)*bc, :]\n",
    "\n",
    "        for j in range(tr):\n",
    "            # load in q, o, m, l\n",
    "            Q_shared[:, :] = Q[j*br:(j+1)*br, :]\n",
    "            # if i == 0: print(f'Q: {Q_shared}')\n",
    "            O_shared[:, :] = O[j*br:(j+1)*br, :]\n",
    "            m_shared[:, :] = m[j*br:(j+1)*br].unsqueeze(-1)\n",
    "            l_shared[:, :] = l[j*br:(j+1)*br].unsqueeze(-1)\n",
    "            \n",
    "            S = Q_shared @ K_shared\n",
    "\n",
    "            # get row-wise softmax statistics\n",
    "            mt = S.max(dim=1).values.reshape(-1, 1)\n",
    "\n",
    "            Pt = torch.exp(S - mt)\n",
    "            lt = Pt.sum(dim=1).reshape(-1, 1)\n",
    "\n",
    "            # compute new statistics\n",
    "            m_new = torch.max(mt, m_shared)\n",
    "            l_new = (torch.exp(m_shared - m_new) * l_shared) + (torch.exp(mt - m_new) * lt)\n",
    "\n",
    "\n",
    "            # update chunk of output\n",
    "            O_new = (l_shared * torch.exp(m_shared - m_new) * O_shared + torch.exp(mt - m_new) * Pt @ V_shared) / l_new\n",
    "            O[j*br:(j+1)*br, :] = O_new\n",
    "            \n",
    "            m[j*br:(j+1)*br] = m_new.flatten()\n",
    "            l[j*br:(j+1)*br] = l_new.flatten()\n",
    "\n",
    "    return O\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[5.7188, 1.9053, 0.9224,  ..., 2.7441, 8.8125, 5.1797],\n",
       "         [9.4453, 1.5811, 3.0957,  ..., 2.2012, 4.0469, 3.1641],\n",
       "         [5.3516, 2.8887, 4.3047,  ..., 3.9160, 0.3467, 9.6953],\n",
       "         ...,\n",
       "         [1.6982, 7.3516, 5.7500,  ..., 5.4141, 8.0547, 2.2109],\n",
       "         [7.9922, 7.8477, 9.7422,  ..., 1.3555, 9.0938, 5.0977],\n",
       "         [5.7188, 6.3359, 8.4141,  ..., 6.4102, 0.7988, 9.5391]],\n",
       "        device='cuda:0', dtype=torch.float16),\n",
       " tensor([[6.0898, 5.8828, 2.2285,  ..., 9.0312, 0.1387, 5.3438],\n",
       "         [4.9258, 2.2617, 8.1641,  ..., 8.6641, 2.4609, 0.3540],\n",
       "         [9.9297, 1.9629, 9.6797,  ..., 1.0459, 9.9609, 2.9980],\n",
       "         ...,\n",
       "         [1.0469, 7.7656, 8.8984,  ..., 1.7734, 1.9453, 0.4707],\n",
       "         [8.2578, 5.8555, 5.2539,  ..., 4.5078, 8.3281, 8.9531],\n",
       "         [5.5117, 4.9258, 3.1738,  ..., 9.5859, 6.6758, 2.5801]],\n",
       "        device='cuda:0', dtype=torch.float16),\n",
       " tensor([[5.6250, 2.0410, 8.8438,  ..., 5.9688, 8.1719, 9.8750],\n",
       "         [3.6934, 6.8398, 6.1133,  ..., 9.3438, 4.2930, 9.4688],\n",
       "         [3.9551, 5.7266, 9.6250,  ..., 8.3047, 1.2734, 9.0547],\n",
       "         ...,\n",
       "         [7.0508, 9.7500, 5.4375,  ..., 1.8574, 0.3137, 6.8555],\n",
       "         [4.7617, 1.2031, 4.5469,  ..., 0.8652, 0.5098, 5.2969],\n",
       "         [0.7861, 4.6562, 2.3203,  ..., 4.3398, 7.5078, 9.5938]],\n",
       "        device='cuda:0', dtype=torch.float16))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import sqrt\n",
    "def dumb_attn(Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"equivalent to F.scaled_dot_product_attention(Q, K, V, scale=1)\"\"\"\n",
    "    # P = (Q @ K.T)\n",
    "    # m = P.max(dim=1).values.reshape(-1, 1)\n",
    "    # print((P - m).exp())\n",
    "    # return torch.softmax(Q @ K.T, dim=1) @ V\n",
    "    d = Q.shape[1]\n",
    "    return F.scaled_dot_product_attention(Q, K, V, scale=d**-0.5)\n",
    "\n",
    "\n",
    "# have to support up to like d=4096\n",
    "# we're giving up and only supporting fp16 (like flashattn)\n",
    "Q = torch.rand((8, 128), dtype=torch.float16, device='cuda') * 10\n",
    "K = torch.rand((8, 128), dtype=torch.float16, device='cuda') * 10\n",
    "V = torch.rand((8, 128), dtype=torch.float16, device='cuda') * 10\n",
    "\n",
    "\n",
    "# Q = torch.randint(1, 9, (8, 4), device='cuda').to(torch.float16)\n",
    "# K = torch.randint(1, 9, (8, 4), device='cuda').to(torch.float16)\n",
    "# V = torch.randint(1, 9, (8, 4), device='cuda').to(torch.float16)\n",
    "\n",
    "# Q = torch.tensor([[0., 1.],\n",
    "#          [2., 3.]])\n",
    "\n",
    "# K = torch.tensor([[0., 1.5],\n",
    "#          [2., 3.]])\n",
    "\n",
    "# V = torch.tensor([[1., 0.],\n",
    "#          [0., 1.]])\n",
    "Q, K, V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/seb/Code/flash-attention/rocm/flash_attention.hip -> /home/seb/Code/flash-attention/rocm/flash_attention_hip.hip [ok]\n",
      "Total number of unsupported CUDA function calls: 0\n",
      "\n",
      "\n",
      "Total number of replaced kernel launches: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The input conditions for extension module m have changed. Bumping to version 2 and re-building as m_v2...\n",
      "\u001b[92mSuccessfully preprocessed all matching files.\u001b[0m\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file build/build.ninja...\n",
      "Building extension module m_v2...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/2] /opt/rocm-6.1.3/bin/hipcc  -DWITH_HIP -DTORCH_EXTENSION_NAME=m_v2 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/seb/Code/pyenvs/rocmenv/lib/python3.12/site-packages/torch/include -isystem /home/seb/Code/pyenvs/rocmenv/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -isystem /home/seb/Code/pyenvs/rocmenv/lib/python3.12/site-packages/torch/include/TH -isystem /home/seb/Code/pyenvs/rocmenv/lib/python3.12/site-packages/torch/include/THC -isystem /home/seb/Code/pyenvs/rocmenv/lib/python3.12/site-packages/torch/include/THH -isystem /opt/rocm-6.1.3/include -isystem /home/seb/miniconda3/include/python3.12 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 --offload-arch=\"gfx1100\" -fPIC -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -DCUDA_HAS_FP16=1 -D__HIP_NO_HALF_OPERATORS__=1 -D__HIP_NO_HALF_CONVERSIONS__=1 -fno-gpu-rdc -c /home/seb/Code/flash-attention/rocm/flash_attention_hip.hip -o flash_attention_hip.cuda.o \n",
      "/home/seb/Code/flash-attention/rocm/flash_attention_hip.hip:304:5: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  304 |     hipGetDeviceProperties(&props, 0);\n",
      "      |     ^~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~~\n",
      "/opt/rocm-6.1.3/include/hip/hip_runtime_api.h:91:32: note: expanded from macro 'hipGetDeviceProperties'\n",
      "   91 | #define hipGetDeviceProperties hipGetDevicePropertiesR0600\n",
      "      |                                ^~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/home/seb/Code/flash-attention/rocm/flash_attention_hip.hip:336:30: warning: format specifies type 'int' but the argument has type 'size_t' (aka 'unsigned long') [-Wformat]\n",
      "  336 |     printf(\"tc=%d, tr=%d\\n\", tc, tr);\n",
      "      |                ~~            ^~\n",
      "      |                %zu\n",
      "/home/seb/Code/flash-attention/rocm/flash_attention_hip.hip:336:34: warning: format specifies type 'int' but the argument has type 'size_t' (aka 'unsigned long') [-Wformat]\n",
      "  336 |     printf(\"tc=%d, tr=%d\\n\", tc, tr);\n",
      "      |                       ~~         ^~\n",
      "      |                       %zu\n",
      "3 warnings generated when compiling for gfx1100.\n",
      "/home/seb/Code/flash-attention/rocm/flash_attention_hip.hip:304:5: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  304 |     hipGetDeviceProperties(&props, 0);\n",
      "      |     ^~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~~\n",
      "/opt/rocm-6.1.3/include/hip/hip_runtime_api.h:91:32: note: expanded from macro 'hipGetDeviceProperties'\n",
      "   91 | #define hipGetDeviceProperties hipGetDevicePropertiesR0600\n",
      "      |                                ^~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/home/seb/Code/flash-attention/rocm/flash_attention_hip.hip:336:30: warning: format specifies type 'int' but the argument has type 'size_t' (aka 'unsigned long') [-Wformat]\n",
      "  336 |     printf(\"tc=%d, tr=%d\\n\", tc, tr);\n",
      "      |                ~~            ^~\n",
      "      |                %zu\n",
      "/home/seb/Code/flash-attention/rocm/flash_attention_hip.hip:336:34: warning: format specifies type 'int' but the argument has type 'size_t' (aka 'unsigned long') [-Wformat]\n",
      "  336 |     printf(\"tc=%d, tr=%d\\n\", tc, tr);\n",
      "      |                       ~~         ^~\n",
      "      |                       %zu\n",
      "3 warnings generated when compiling for host.\n",
      "[2/2] c++ flash_attention_hip.cuda.o -shared -L/home/seb/Code/pyenvs/rocmenv/lib/python3.12/site-packages/torch/lib -lc10 -lc10_hip -ltorch_cpu -ltorch_hip -ltorch -ltorch_python -L/opt/rocm-6.1.3/lib -lamdhip64 -o m_v2.so\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module m_v2...\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.cpp_extension import load\n",
    "module = load(\n",
    "    name='m',\n",
    "    # sources=['cuda/main.cpp', 'cuda/flash_attention.cu'],\n",
    "    sources=['rocm/flash_attention.hip',],\n",
    "    extra_cflags=['--offload-arch=\"gfx1100\"',],\n",
    "    build_directory='build',\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Shared Memory per Block: 64 KB\n",
      "  Shared Memory Banks: 65536\n",
      "  Warp Size: 32\n",
      "  Max Threads per Block: 1024\n",
      "  Max Threads per Multiprocessor: 2048\n",
      "\n",
      "tc=1, tr=1\n",
      "scaling QK^T by 0.0883789\n",
      "Launching kernel with smem size 8320\n",
      "[[0.5054, 0.1683, 0.0815, 0.0861, 0.0204, 0.2272, 0.0088, 0.5264, 0.5474, 0.2737, 0.6104, 0.6304, 0.5884, 0.2969, 0.2302, 0.4695, 0.1288, 0.4771, 0.5322, 0.2292, 0.1032, 0.8672, 0.6909, 0.6782, 0.8740, 0.5117, 0.8760, 0.3018, 0.6792, 0.0432, 0.6548, 0.4841, 0.5815, 0.2654, 0.1848, 0.7524, 0.1581, 0.6875, 0.0547, 0.4585, 0.2417, 0.6685, 0.8350, 0.8706, 0.2649, 0.6147, 0.1885, 0.6489, 0.5967, 0.6831, 0.2634, 0.6802, 0.5073, 0.0037, 0.4426, 0.0249, 0.6099, 0.1449, 0.6792, 0.8193, 0.1809, 0.5127, 0.4836, 0.5000, 0.5903, 0.6416, 0.7749, 0.1964, 0.0405, 0.4458, 0.6196, 0.0450, 0.7866, 0.3191, 0.7090, 0.0061, 0.8452, 0.2219, 0.0727, 0.4958, 0.0664, 0.1964, 0.2603, 0.7485, 0.0652, 0.7759, 0.3967, 0.5562, 0.7466, 0.5923, 0.5088, 0.7300, 0.7759, 0.1827, 0.0218, 0.0089, 0.7554, 0.3562, 0.4377, 0.3208, 0.2959, 0.1138, 0.6938, 0.0127, 0.5542, 0.5239, 0.6123, 0.7061, 0.3645, 0.5693, 0.1036, 0.2549, 0.2085, 0.3154, 0.8774, 0.4900, 0.2500, 0.1418, 0.0578, 0.8657, 0.5425, 0.3618, 0.0668, 0.2235, 0.5264, 0.2426, 0.7788, 0.4578, ]\n",
      "[0.8350, 0.1398, 0.2737, 0.0179, 0.4128, 0.8789, 0.4905, 0.8301, 0.5542, 0.3943, 0.5830, 0.0547, 0.7749, 0.1302, 0.8403, 0.2039, 0.7788, 0.3960, 0.3013, 0.4485, 0.7192, 0.8208, 0.1061, 0.3433, 0.7705, 0.1488, 0.3254, 0.8257, 0.7236, 0.8135, 0.7886, 0.2040, 0.5679, 0.3281, 0.7417, 0.3738, 0.6172, 0.6318, 0.4934, 0.4475, 0.7554, 0.7349, 0.2438, 0.8320, 0.7173, 0.0203, 0.3828, 0.6509, 0.6538, 0.3862, 0.0696, 0.7734, 0.0826, 0.8506, 0.6167, 0.5908, 0.3303, 0.0651, 0.4944, 0.5522, 0.2438, 0.1516, 0.8657, 0.2844, 0.4280, 0.5005, 0.1937, 0.0667, 0.3455, 0.1709, 0.4868, 0.5596, 0.4077, 0.1377, 0.5181, 0.4329, 0.1575, 0.1339, 0.5264, 0.5493, 0.2123, 0.2527, 0.0863, 0.8252, 0.7856, 0.4900, 0.3030, 0.6304, 0.8652, 0.1115, 0.6240, 0.6909, 0.5063, 0.5332, 0.3818, 0.1727, 0.4744, 0.2837, 0.5903, 0.1442, 0.8057, 0.7153, 0.5957, 0.7383, 0.7505, 0.4250, 0.1382, 0.5591, 0.4678, 0.6553, 0.0967, 0.3916, 0.2761, 0.3943, 0.4539, 0.5918, 0.1675, 0.2607, 0.2288, 0.7832, 0.8789, 0.6694, 0.5854, 0.5625, 0.7021, 0.1946, 0.3577, 0.2795, ]\n",
      "[0.4729, 0.2554, 0.3804, 0.8726, 0.2974, 0.3081, 0.7393, 0.6924, 0.5625, 0.8193, 0.4717, 0.2496, 0.8350, 0.6538, 0.3501, 0.8174, 0.7051, 0.0726, 0.6528, 0.2268, 0.3494, 0.7637, 0.8350, 0.3054, 0.7075, 0.6333, 0.6221, 0.8721, 0.6924, 0.4941, 0.1027, 0.4268, 0.3962, 0.4294, 0.8726, 0.1752, 0.4517, 0.2522, 0.5830, 0.8091, 0.7251, 0.6401, 0.4954, 0.5684, 0.2427, 0.2891, 0.6406, 0.3074, 0.3411, 0.2380, 0.7607, 0.8569, 0.3345, 0.0734, 0.8125, 0.2396, 0.2727, 0.2227, 0.1246, 0.0262, 0.2023, 0.3708, 0.3796, 0.4751, 0.8818, 0.7222, 0.0731, 0.5850, 0.1240, 0.5459, 0.8223, 0.5425, 0.6938, 0.4917, 0.3347, 0.3611, 0.2279, 0.6899, 0.0855, 0.8223, 0.4871, 0.8418, 0.3352, 0.8105, 0.5542, 0.8760, 0.2732, 0.8569, 0.4700, 0.2172, 0.1389, 0.0153, 0.4404, 0.6572, 0.5986, 0.4954, 0.4316, 0.7339, 0.2891, 0.3728, 0.3179, 0.5063, 0.3801, 0.5972, 0.2361, 0.0148, 0.5215, 0.7690, 0.6992, 0.7568, 0.8306, 0.5483, 0.3601, 0.0348, 0.6440, 0.6606, 0.6831, 0.3857, 0.0802, 0.5693, 0.6685, 0.5967, 0.4380, 0.1078, 0.6890, 0.3462, 0.0306, 0.8569, ]\n",
      "[0.7949, 0.8774, 0.5483, 0.3411, 0.7109, 0.3494, 0.7148, 0.0814, 0.4829, 0.4919, 0.4729, 0.6724, 0.8555, 0.0799, 0.3425, 0.7876, 0.8799, 0.2837, 0.4758, 0.1036, 0.5381, 0.2781, 0.0896, 0.0029, 0.5107, 0.0573, 0.0246, 0.2493, 0.7422, 0.2759, 0.1556, 0.6797, 0.2450, 0.5352, 0.3438, 0.3953, 0.0233, 0.1329, 0.3552, 0.0673, 0.5898, 0.6323, 0.3611, 0.7588, 0.2211, 0.7153, 0.3862, 0.5752, 0.2056, 0.0876, 0.0515, 0.1220, 0.1713, 0.2991, 0.0235, 0.6455, 0.0241, 0.5532, 0.5200, 0.2018, 0.0981, 0.3364, 0.0134, 0.1462, 0.8359, 0.2217, 0.7319, 0.5142, 0.8057, 0.4888, 0.1256, 0.0247, 0.4250, 0.4570, 0.7339, 0.1477, 0.5322, 0.1788, 0.2329, 0.6133, 0.6494, 0.0555, 0.2764, 0.6489, 0.6807, 0.4175, 0.0249, 0.7983, 0.4275, 0.0290, 0.7256, 0.4275, 0.5537, 0.1105, 0.2769, 0.1613, 0.4370, 0.0342, 0.5098, 0.4758, 0.5840, 0.0746, 0.6958, 0.4287, 0.2465, 0.0080, 0.6099, 0.8276, 0.1481, 0.5474, 0.5308, 0.1190, 0.4910, 0.2393, 0.3530, 0.3586, 0.7920, 0.5884, 0.5884, 0.5537, 0.6289, 0.1346, 0.7959, 0.4805, 0.7485, 0.1842, 0.4043, 0.6187, ]\n",
      "[0.6802, 0.4744, 0.7915, 0.6309, 0.2268, 0.7241, 0.5635, 0.4751, 0.6323, 0.8623, 0.4663, 0.6597, 0.2878, 0.4558, 0.3594, 0.7207, 0.2085, 0.4900, 0.2832, 0.7607, 0.8140, 0.4778, 0.1016, 0.0428, 0.7817, 0.7520, 0.4363, 0.3762, 0.0828, 0.1783, 0.6079, 0.2983, 0.1575, 0.0626, 0.1243, 0.6924, 0.6934, 0.0994, 0.1048, 0.0156, 0.5405, 0.8276, 0.3936, 0.2410, 0.8486, 0.8755, 0.2583, 0.3123, 0.4629, 0.3489, 0.8755, 0.6948, 0.4370, 0.8154, 0.7603, 0.0530, 0.0026, 0.2294, 0.8569, 0.0638, 0.3232, 0.5371, 0.3474, 0.0690, 0.7520, 0.6919, 0.5542, 0.4419, 0.3159, 0.2004, 0.8677, 0.8105, 0.1372, 0.5820, 0.8052, 0.2087, 0.8232, 0.0238, 0.5854, 0.4216, 0.5332, 0.6782, 0.1377, 0.1536, 0.7041, 0.8823, 0.5083, 0.8667, 0.2494, 0.5566, 0.4475, 0.0353, 0.3296, 0.5142, 0.8120, 0.1914, 0.5024, 0.7651, 0.8325, 0.7012, 0.2778, 0.3628, 0.2230, 0.7915, 0.6299, 0.0533, 0.2537, 0.6597, 0.2708, 0.6440, 0.3562, 0.4658, 0.6035, 0.2910, 0.1801, 0.2500, 0.6973, 0.6899, 0.8340, 0.2289, 0.6968, 0.4094, 0.7900, 0.7842, 0.8320, 0.4543, 0.1101, 0.3252, ]\n",
      "[0.1501, 0.6499, 0.5083, 0.0609, 0.1399, 0.1345, 0.2827, 0.4387, 0.2008, 0.3796, 0.5049, 0.7231, 0.8540, 0.2529, 0.2607, 0.8438, 0.8633, 0.8325, 0.5210, 0.7393, 0.0366, 0.2607, 0.4136, 0.6709, 0.2859, 0.6357, 0.0345, 0.7510, 0.2430, 0.4575, 0.1931, 0.6406, 0.2146, 0.8784, 0.0257, 0.3296, 0.1968, 0.7188, 0.4827, 0.3276, 0.1125, 0.6890, 0.7104, 0.0829, 0.5405, 0.6836, 0.4504, 0.6323, 0.5947, 0.2788, 0.5889, 0.6626, 0.4902, 0.7803, 0.3010, 0.6040, 0.2058, 0.2289, 0.0096, 0.0119, 0.1110, 0.5381, 0.6123, 0.2886, 0.3811, 0.7773, 0.5322, 0.2507, 0.4978, 0.4387, 0.5215, 0.0805, 0.6338, 0.3975, 0.7349, 0.4539, 0.0050, 0.6665, 0.8638, 0.2754, 0.8477, 0.0917, 0.8691, 0.1992, 0.8457, 0.1440, 0.0676, 0.5352, 0.6104, 0.0710, 0.4128, 0.3247, 0.2244, 0.1613, 0.1267, 0.4543, 0.7739, 0.1009, 0.5190, 0.2218, 0.8501, 0.5791, 0.4277, 0.5522, 0.1151, 0.6802, 0.2512, 0.6440, 0.2666, 0.0776, 0.2654, 0.6904, 0.5317, 0.8223, 0.8105, 0.7969, 0.7300, 0.4380, 0.5923, 0.5796, 0.2661, 0.4033, 0.8789, 0.5835, 0.7866, 0.4785, 0.7119, 0.1954, ]\n",
      "[0.7065, 0.6934, 0.8608, 0.7188, 0.1057, 0.5903, 0.3835, 0.6455, 0.3152, 0.4661, 0.0454, 0.1539, 0.1553, 0.3438, 0.0175, 0.4253, 0.2771, 0.2529, 0.4678, 0.6006, 0.7954, 0.1868, 0.3328, 0.6587, 0.0500, 0.0801, 0.4094, 0.7832, 0.4226, 0.1969, 0.6748, 0.7856, 0.1057, 0.4563, 0.7983, 0.6553, 0.0246, 0.3120, 0.5923, 0.2803, 0.1170, 0.6416, 0.1500, 0.0862, 0.3213, 0.8052, 0.0865, 0.3950, 0.6616, 0.0000, 0.3220, 0.2434, 0.3962, 0.0494, 0.0540, 0.2163, 0.8267, 0.7393, 0.0436, 0.6426, 0.7485, 0.7559, 0.0079, 0.3628, 0.2367, 0.0511, 0.4011, 0.3306, 0.6758, 0.5923, 0.3027, 0.0802, 0.8501, 0.4180, 0.2800, 0.1234, 0.6299, 0.4639, 0.0285, 0.7124, 0.5459, 0.3621, 0.2661, 0.8569, 0.0330, 0.0069, 0.7554, 0.8726, 0.4438, 0.7485, 0.3560, 0.5024, 0.1500, 0.1544, 0.3352, 0.4778, 0.4143, 0.1217, 0.8604, 0.8823, 0.3279, 0.0164, 0.1322, 0.6011, 0.5864, 0.6255, 0.6191, 0.4043, 0.0980, 0.3928, 0.1130, 0.6841, 0.4399, 0.1265, 0.0377, 0.0818, 0.5068, 0.7915, 0.2554, 0.3184, 0.7192, 0.8301, 0.3367, 0.7056, 0.5176, 0.1198, 0.8037, 0.4504, ]\n",
      "[0.5054, 0.5601, 0.7437, 0.2778, 0.1472, 0.4817, 0.8521, 0.6299, 0.2974, 0.1659, 0.1992, 0.1483, 0.3467, 0.3191, 0.4180, 0.6758, 0.6768, 0.4917, 0.2200, 0.4119, 0.0901, 0.5645, 0.2607, 0.8105, 0.8135, 0.6665, 0.0245, 0.6768, 0.6094, 0.5054, 0.5215, 0.2661, 0.2385, 0.1926, 0.8203, 0.6865, 0.2937, 0.3621, 0.4680, 0.2018, 0.2363, 0.2488, 0.0841, 0.2749, 0.2759, 0.2384, 0.5747, 0.8823, 0.8555, 0.2479, 0.8716, 0.7500, 0.3577, 0.3088, 0.5815, 0.7153, 0.7886, 0.0377, 0.6577, 0.3220, 0.7705, 0.4475, 0.6406, 0.6621, 0.7651, 0.5420, 0.1213, 0.4800, 0.5830, 0.6045, 0.5044, 0.6729, 0.1471, 0.4675, 0.7241, 0.0930, 0.2710, 0.2021, 0.8271, 0.5791, 0.7358, 0.7622, 0.2661, 0.6733, 0.8257, 0.0562, 0.1814, 0.3901, 0.2781, 0.0406, 0.1581, 0.4805, 0.5542, 0.8271, 0.2451, 0.3687, 0.6719, 0.4458, 0.0434, 0.5356, 0.3174, 0.5356, 0.0129, 0.7759, 0.4661, 0.2181, 0.1615, 0.5010, 0.3853, 0.1598, 0.8486, 0.2350, 0.3909, 0.7993, 0.0481, 0.1802, 0.7310, 0.1746, 0.6851, 0.5264, 0.6182, 0.6650, 0.8809, 0.5107, 0.1041, 0.5664, 0.0706, 0.8433, ]]\n",
      "\n",
      "P_s before shifted exp:\n",
      "[[279.5000, 247.8750, 294.2500, 304.0000, 260.7500, 251.1250, 262.7500, 313.7500, ]\n",
      "[325.5000, 289.5000, 324.2500, 322.5000, 292.5000, 301.2500, 280.7500, 339.0000, ]\n",
      "[326.0000, 288.7500, 327.7500, 320.0000, 319.5000, 301.0000, 305.0000, 335.0000, ]\n",
      "[266.7500, 239.7500, 286.7500, 283.0000, 252.7500, 244.3750, 256.2500, 285.0000, ]\n",
      "[298.7500, 284.0000, 319.7500, 323.7500, 311.2500, 304.7500, 294.0000, 347.5000, ]\n",
      "[283.5000, 272.5000, 313.7500, 311.7500, 284.0000, 275.5000, 280.7500, 331.2500, ]\n",
      "[272.2500, 245.5000, 294.5000, 277.2500, 262.2500, 251.8750, 280.2500, 304.2500, ]\n",
      "[301.2500, 268.5000, 309.5000, 303.0000, 278.5000, 291.0000, 289.5000, 321.2500, ]]\n",
      "\n",
      "P_s after shifted exp:\n",
      "[[0.0000, 0.0000, 0.0000, 0.0001, 0.0000, 0.0000, 0.0000, 1.0000, ]\n",
      "[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, ]\n",
      "[0.0001, 0.0000, 0.0007, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, ]\n",
      "[0.0000, 0.0000, 1.0000, 0.0235, 0.0000, 0.0000, 0.0000, 0.1738, ]\n",
      "[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, ]\n",
      "[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, ]\n",
      "[0.0000, 0.0000, 0.0001, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, ]\n",
      "[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, ]]\n",
      "\n",
      "p_s should be the same as before\n",
      "[[0.0000, 0.0000, 0.0000, 0.0001, 0.0000, 0.0000, 0.0000, 1.0000, ]\n",
      "[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, ]\n",
      "[0.0001, 0.0000, 0.0007, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, ]\n",
      "[0.0000, 0.0000, 1.0000, 0.0235, 0.0000, 0.0000, 0.0000, 0.1738, ]\n",
      "[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, ]\n",
      "[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, ]\n",
      "[0.0000, 0.0000, 0.0001, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, ]\n",
      "[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, ]]\n",
      "\n",
      "V_s is:\n",
      "[[5.6250, 2.0410, 8.8438, 3.0254, 5.2539, 2.6914, 7.0117, 8.4453, 6.8320, 6.9219, 3.0742, 0.2688, 0.3145, 2.3457, 6.5820, 8.4297, 7.4453, 9.6016, 8.9219, 8.6875, 1.5049, 6.3672, 7.2656, 2.0820, 5.1562, 9.4844, 3.8672, 2.6465, 7.4453, 0.9385, 7.1094, 3.5039, 3.4844, 2.5625, 1.4141, 6.2734, 8.4219, 2.4258, 8.6797, 3.3242, 3.4766, 7.2227, 5.8242, 7.2344, 8.5234, 6.4648, 3.7129, 8.7656, 5.6055, 8.8125, 0.8018, 9.5234, 4.6250, 5.7031, 5.1797, 5.2305, 7.9336, 8.0078, 3.7227, 3.6211, 2.2344, 8.9922, 4.4492, 5.6836, 5.1250, 2.3906, 8.9766, 3.3867, 5.9219, 7.8867, 2.5684, 5.3516, 8.6016, 0.0510, 7.9727, 9.8516, 9.0312, 3.9141, 3.7520, 4.4297, 2.2891, 0.9102, 6.5078, 9.6250, 5.7891, 5.9062, 1.9824, 7.9922, 1.6807, 6.8516, 5.9336, 1.8477, 9.7656, 4.3594, 4.0938, 4.6289, 6.8945, 4.3164, 3.6133, 0.8984, 0.9951, 3.5957, 7.7109, 5.7266, 5.9180, 9.3281, 6.9453, 9.2344, 8.9141, 0.2559, 3.5820, 9.1953, 2.3457, 7.3672, 7.0391, 0.2300, 3.3223, 8.1328, 3.8574, 1.0957, 7.0312, 1.9121, 9.9766, 8.3047, 3.3086, 5.9688, 8.1719, 9.8750, ]\n",
      "[3.6934, 6.8398, 6.1133, 0.9268, 5.9297, 7.0859, 3.4844, 8.1094, 3.7148, 6.4648, 0.9297, 8.2188, 6.3711, 8.1875, 6.2031, 4.4414, 1.1836, 6.6719, 9.7969, 3.5449, 0.4880, 2.1973, 0.2416, 9.5391, 0.9429, 1.6387, 9.4297, 2.1816, 9.1328, 7.2344, 5.4766, 6.0469, 0.3042, 2.2031, 9.7188, 1.0342, 3.9609, 6.2891, 9.2266, 5.8164, 1.0918, 8.2109, 6.7812, 3.6016, 3.2832, 4.6289, 3.4746, 8.4219, 9.0781, 1.3340, 6.7266, 9.7500, 6.9961, 2.7129, 9.8984, 3.3828, 5.2031, 0.7842, 7.2695, 4.3477, 9.8047, 4.7383, 0.8179, 6.8203, 4.2617, 3.2109, 6.7773, 1.6943, 8.4688, 8.0469, 5.9180, 7.8359, 5.1719, 4.2891, 8.9297, 4.7344, 5.2656, 6.5859, 5.7773, 0.9004, 2.9414, 3.0625, 5.9570, 8.1953, 3.3086, 0.1918, 7.0117, 3.8770, 3.6816, 2.6543, 3.7129, 5.4531, 4.4883, 8.1094, 5.8828, 7.4805, 9.7422, 2.5117, 0.7520, 6.2109, 3.8320, 0.5342, 9.9531, 7.5586, 8.9531, 2.2227, 9.2500, 2.5039, 0.4758, 2.2031, 5.6406, 3.2031, 7.7930, 3.8242, 0.9302, 3.7129, 7.0742, 8.0469, 2.0332, 4.5078, 7.7305, 6.1484, 4.8359, 8.4297, 6.0977, 9.3438, 4.2930, 9.4688, ]\n",
      "[3.9551, 5.7266, 9.6250, 2.9316, 6.0977, 7.7578, 8.5078, 7.7734, 6.5469, 5.3281, 2.3867, 2.4375, 1.2578, 4.0820, 0.2732, 7.2695, 9.4844, 2.0234, 1.1045, 9.5703, 6.3750, 5.6680, 2.1992, 6.9570, 6.7773, 9.7344, 3.7285, 3.9043, 8.1406, 7.6641, 0.6489, 4.5859, 9.6250, 7.9609, 5.7266, 0.3501, 7.2227, 2.2852, 0.9590, 2.2578, 5.5117, 6.0625, 2.2422, 7.5547, 5.8750, 2.7363, 2.2109, 4.5352, 8.3359, 6.2656, 9.8828, 7.1328, 1.0547, 0.9170, 9.8281, 6.7656, 3.8984, 5.1094, 0.2988, 3.4180, 7.6914, 6.5469, 6.8945, 6.3359, 4.2539, 4.9492, 2.1094, 1.4541, 5.3047, 6.5820, 4.9023, 7.9648, 0.0533, 0.9771, 9.0938, 7.5352, 3.4766, 2.1289, 6.1094, 9.3594, 6.7188, 4.5977, 5.8672, 2.8633, 4.1836, 6.7383, 8.1641, 7.9883, 1.7461, 4.8633, 7.4766, 1.6895, 2.8164, 5.8125, 3.7695, 1.0273, 3.7148, 3.9746, 8.2422, 6.9492, 1.9287, 4.2969, 9.7812, 1.4492, 4.8867, 2.4746, 2.3887, 0.1782, 8.0312, 7.9531, 4.3398, 2.0742, 6.6328, 3.0547, 2.8105, 2.3945, 2.0781, 6.5391, 0.6206, 7.7656, 9.8906, 0.6436, 8.7969, 2.0176, 6.5039, 8.3047, 1.2734, 9.0547, ]\n",
      "[1.1611, 9.4688, 7.1719, 1.6953, 2.7129, 8.6797, 4.8086, 9.9141, 3.8301, 7.2734, 0.9790, 1.9277, 4.5898, 7.1797, 8.7812, 4.0586, 8.3984, 8.5625, 9.1172, 7.8047, 9.8906, 9.6250, 9.9688, 9.9141, 0.1143, 8.3516, 2.9277, 9.5781, 9.5781, 4.8125, 1.2217, 9.6562, 7.8477, 0.4023, 2.7832, 2.2422, 2.1797, 5.3594, 8.5703, 5.7344, 9.8828, 9.9844, 6.9141, 0.7637, 6.2891, 8.2188, 0.7480, 7.0859, 2.5117, 5.3281, 4.0430, 3.7285, 4.2383, 8.1328, 9.1719, 5.9297, 4.0391, 4.2930, 2.0605, 2.3672, 7.7422, 2.4688, 1.7666, 2.3945, 5.9609, 8.3594, 3.7461, 1.0342, 4.6758, 6.8359, 7.9922, 9.8594, 4.1172, 0.1990, 1.0898, 4.8906, 1.0215, 6.1133, 4.3203, 5.5820, 6.7344, 4.9609, 9.0156, 4.5156, 7.6094, 3.4746, 6.0352, 0.8745, 1.8242, 3.7812, 3.4746, 6.0312, 4.0625, 5.6797, 0.6738, 6.3281, 5.7969, 4.0078, 8.1875, 0.1214, 6.0742, 3.3750, 1.3926, 1.5537, 2.8418, 1.2939, 6.5039, 4.8203, 5.6641, 0.0180, 4.4023, 0.8936, 0.3591, 4.3320, 0.6152, 2.8945, 0.8599, 4.5039, 5.1016, 2.3711, 1.7637, 9.8047, 5.7461, 2.2969, 4.2227, 5.3516, 2.3457, 8.0781, ]\n",
      "[6.6875, 3.3711, 1.1035, 1.4512, 5.6406, 5.7812, 3.4473, 4.1094, 4.8867, 0.3176, 7.6562, 3.5938, 8.6016, 2.4492, 8.8438, 7.7383, 2.3594, 0.3784, 1.8604, 7.0742, 9.5703, 1.4375, 6.6562, 9.1328, 4.0234, 4.7852, 0.4246, 5.0312, 4.1094, 1.6748, 8.0391, 9.2188, 5.1562, 2.2383, 8.8516, 0.0674, 9.5469, 6.2148, 8.4531, 5.1523, 8.5781, 9.8984, 9.1875, 1.0449, 2.6152, 5.7266, 5.9922, 4.5391, 1.4697, 9.7266, 8.3438, 6.9297, 8.7578, 3.6289, 9.4375, 0.9980, 1.7988, 0.2969, 0.4512, 4.8086, 9.6719, 4.6172, 7.5781, 3.4375, 1.6904, 9.2812, 6.2539, 0.1561, 9.1172, 8.2188, 7.6484, 4.3398, 2.7031, 9.4609, 0.1692, 9.5156, 0.9282, 7.2891, 8.9375, 2.3418, 5.4922, 5.2031, 0.3748, 6.0586, 7.7188, 9.8750, 0.9570, 8.0156, 7.0117, 5.0156, 8.3594, 2.9258, 0.6143, 6.1250, 0.3723, 2.3418, 4.3906, 3.2852, 3.7656, 9.9531, 9.7344, 5.3086, 0.5679, 8.5312, 9.2812, 1.9541, 4.0703, 1.1973, 3.1816, 1.6582, 9.9219, 2.6523, 4.3203, 6.3242, 8.7344, 2.3770, 6.6445, 6.3594, 0.0718, 3.0703, 5.9297, 2.4395, 6.0898, 5.7852, 7.1953, 5.4258, 6.3242, 9.2578, ]\n",
      "[7.0508, 9.7500, 5.4375, 4.3203, 0.8643, 5.1523, 7.3477, 3.5566, 5.4453, 2.5996, 8.6016, 3.3496, 4.9531, 9.3438, 4.1719, 7.1836, 2.0156, 2.6719, 9.7031, 9.0469, 3.3125, 5.7734, 5.0156, 4.5859, 5.3477, 0.8936, 3.8906, 0.0181, 7.0859, 0.9004, 8.9141, 1.6094, 1.0312, 8.6406, 8.5625, 5.0000, 7.0703, 9.2656, 9.8672, 6.5156, 2.3809, 9.0312, 2.4961, 1.0811, 7.1172, 0.1432, 5.2773, 7.1133, 6.7148, 1.6748, 8.0547, 9.2109, 3.4355, 8.4297, 9.9609, 4.8359, 5.0078, 9.7500, 8.1172, 2.7109, 2.6211, 8.5938, 0.9937, 6.6367, 9.8516, 9.4688, 6.5938, 8.2734, 8.9375, 8.7812, 7.0117, 0.3604, 5.9727, 5.1328, 9.3906, 3.2773, 6.2695, 2.8672, 3.6367, 9.0938, 6.8359, 1.6270, 3.9648, 8.1094, 2.9277, 3.9355, 0.6533, 3.9531, 5.0625, 5.1133, 6.5547, 8.9922, 6.2852, 9.2812, 1.6846, 6.8320, 2.5996, 7.4062, 7.0898, 7.8711, 1.6406, 8.7188, 2.4297, 8.1484, 5.4922, 6.0352, 4.2422, 8.0312, 7.9883, 8.2500, 1.5977, 1.5049, 3.0684, 3.8359, 5.6953, 5.3945, 2.6758, 3.6992, 0.9150, 1.9453, 6.7773, 3.9922, 5.2969, 7.6992, 0.3872, 1.8574, 0.3137, 6.8555, ]\n",
      "[4.7617, 1.2031, 4.5469, 8.3750, 7.2109, 8.2734, 2.0605, 1.6064, 1.1631, 1.8516, 3.3125, 4.7305, 4.7461, 5.6484, 4.2344, 5.3828, 9.9141, 0.2776, 5.7656, 2.8008, 0.9302, 1.5439, 1.0703, 3.4551, 2.4531, 4.1680, 7.8906, 9.9688, 8.7734, 3.9531, 7.3242, 5.9805, 2.8418, 0.3625, 5.6328, 1.7734, 2.7051, 4.0977, 6.8555, 7.1328, 3.5352, 1.1943, 6.6367, 2.8750, 3.6875, 9.3281, 1.9834, 0.3293, 1.8838, 2.6094, 4.2109, 4.6406, 7.7344, 1.6406, 9.5000, 5.3828, 8.2500, 5.7383, 0.0292, 2.9805, 2.5684, 5.7617, 0.0359, 2.9082, 1.5977, 6.5625, 3.1641, 3.6641, 2.9570, 2.3281, 9.5391, 8.6094, 5.4844, 6.4062, 4.3906, 9.1406, 8.0938, 1.3613, 9.5938, 3.9258, 6.0156, 2.0078, 7.2227, 4.3320, 7.6992, 0.0848, 8.4062, 7.6641, 3.0488, 8.3828, 3.7090, 0.7676, 3.3516, 7.7969, 4.8828, 2.9512, 9.2891, 8.5156, 7.4180, 2.5039, 5.5156, 5.7500, 9.7812, 7.2852, 1.1543, 2.5293, 6.3633, 5.7852, 5.6211, 5.8672, 0.1425, 9.3125, 7.0508, 7.3477, 5.7109, 5.2734, 3.6797, 4.3320, 9.1250, 3.4336, 4.1250, 2.1895, 1.9277, 0.9351, 7.4180, 0.8652, 0.5098, 5.2969, ]\n",
      "[0.7861, 4.6562, 2.3203, 9.2891, 2.8105, 7.8281, 8.3984, 1.6152, 3.0508, 2.2676, 0.2000, 9.6953, 7.8906, 7.5977, 5.2031, 9.2500, 1.0635, 7.8672, 3.1660, 2.9727, 8.7422, 8.7422, 6.6328, 4.9531, 1.1055, 9.3125, 4.3945, 5.2695, 0.2415, 1.0684, 8.6250, 1.4160, 6.9570, 9.1328, 6.2578, 0.0890, 7.2578, 3.1367, 6.2148, 6.3711, 4.6914, 7.8945, 4.4727, 8.8594, 4.7578, 0.4551, 9.6719, 8.4141, 8.6406, 1.9883, 8.1562, 2.1914, 9.4375, 9.1094, 6.9570, 6.0078, 4.5742, 4.2500, 9.5625, 9.1250, 0.2098, 5.5117, 0.2205, 7.5156, 5.9961, 1.3105, 9.8438, 8.6406, 5.5547, 1.7686, 8.4844, 1.6035, 2.9023, 2.7363, 9.3906, 9.8516, 6.4219, 2.9727, 3.4805, 2.5020, 1.4395, 1.4023, 8.1719, 1.0010, 3.3887, 5.6641, 4.2031, 0.0190, 8.5938, 8.1406, 6.2148, 4.1484, 6.7383, 8.5469, 9.3594, 2.2891, 2.2578, 0.4714, 3.6953, 3.3379, 4.1445, 6.8047, 8.1562, 0.3928, 4.7305, 7.7266, 0.9580, 3.7930, 5.9453, 4.8164, 5.1211, 8.1797, 1.5635, 7.3125, 9.0547, 7.7969, 3.5723, 0.5371, 4.6797, 4.0312, 0.3787, 6.8594, 8.4219, 6.6836, 9.9219, 4.3398, 7.5078, 9.5938, ]]\n",
      "\n",
      "[[0.7861, 4.6562, 2.3203, 9.2891, 2.8105, 7.8281, 8.3984, 1.6162, 3.0508, 2.2676, 0.2000, 9.6953, 7.8906, 7.5977, 5.2031, 9.2500, 1.0645, 7.8672, 3.1660, 2.9727, 8.7422, 8.7422, 6.6328, 4.9531, 1.1055, 9.3125, 4.3945, 5.2695, 0.2421, 1.0684, 8.6250, 1.4170, 6.9570, 9.1328, 6.2578, 0.0891, 7.2578, 3.1367, 6.2148, 6.3711, 4.6914, 7.8945, 4.4727, 8.8594, 4.7578, 0.4556, 9.6719, 8.4141, 8.6406, 1.9883, 8.1562, 2.1914, 9.4375, 9.1094, 6.9570, 6.0078, 4.5742, 4.2500, 9.5625, 9.1250, 0.2103, 5.5117, 0.2206, 7.5156, 5.9961, 1.3105, 9.8438, 8.6406, 5.5547, 1.7686, 8.4844, 1.6045, 2.9023, 2.7363, 9.3906, 9.8516, 6.4219, 2.9727, 3.4805, 2.5020, 1.4395, 1.4023, 8.1719, 1.0010, 3.3887, 5.6641, 4.2031, 0.0190, 8.5938, 8.1406, 6.2148, 4.1484, 6.7383, 8.5469, 9.3594, 2.2891, 2.2578, 0.4717, 3.6953, 3.3379, 4.1445, 6.8047, 8.1562, 0.3928, 4.7305, 7.7266, 0.9585, 3.7930, 5.9453, 4.8164, 5.1211, 8.1797, 1.5635, 7.3125, 9.0547, 7.7969, 3.5723, 0.5376, 4.6797, 4.0312, 0.3787, 6.8594, 8.4219, 6.6836, 9.9219, 4.3398, 7.5078, 9.5938, ]\n",
      "[0.7861, 4.6562, 2.3203, 9.2891, 2.8105, 7.8281, 8.3984, 1.6152, 3.0508, 2.2676, 0.2000, 9.6953, 7.8906, 7.5977, 5.2031, 9.2500, 1.0635, 7.8672, 3.1660, 2.9727, 8.7422, 8.7422, 6.6328, 4.9531, 1.1055, 9.3125, 4.3945, 5.2695, 0.2415, 1.0684, 8.6250, 1.4160, 6.9570, 9.1328, 6.2578, 0.0890, 7.2578, 3.1367, 6.2148, 6.3711, 4.6914, 7.8945, 4.4727, 8.8594, 4.7578, 0.4551, 9.6719, 8.4141, 8.6406, 1.9883, 8.1562, 2.1914, 9.4375, 9.1094, 6.9570, 6.0078, 4.5742, 4.2500, 9.5625, 9.1250, 0.2098, 5.5117, 0.2205, 7.5156, 5.9961, 1.3105, 9.8438, 8.6406, 5.5547, 1.7686, 8.4844, 1.6035, 2.9023, 2.7363, 9.3906, 9.8516, 6.4219, 2.9727, 3.4805, 2.5020, 1.4395, 1.4023, 8.1719, 1.0010, 3.3887, 5.6641, 4.2031, 0.0190, 8.5938, 8.1406, 6.2148, 4.1484, 6.7383, 8.5469, 9.3594, 2.2891, 2.2578, 0.4714, 3.6953, 3.3379, 4.1445, 6.8047, 8.1562, 0.3928, 4.7305, 7.7266, 0.9580, 3.7930, 5.9453, 4.8164, 5.1211, 8.1797, 1.5635, 7.3125, 9.0547, 7.7969, 3.5723, 0.5371, 4.6797, 4.0312, 0.3787, 6.8594, 8.4219, 6.6836, 9.9219, 4.3398, 7.5078, 9.5938, ]\n",
      "[0.7886, 4.6562, 2.3262, 9.2812, 2.8145, 7.8242, 8.3984, 1.6201, 3.0527, 2.2695, 0.2018, 9.6875, 7.8828, 7.5938, 5.1992, 9.2500, 1.0703, 7.8633, 3.1641, 2.9785, 8.7422, 8.7422, 6.6289, 4.9531, 1.1104, 9.3125, 4.3945, 5.2695, 0.2479, 1.0732, 8.6172, 1.4189, 6.9570, 9.1328, 6.2539, 0.0900, 7.2578, 3.1348, 6.2070, 6.3672, 4.6914, 7.8906, 4.4727, 8.8594, 4.7578, 0.4573, 9.6641, 8.4141, 8.6406, 1.9922, 8.1562, 2.1953, 9.4297, 9.1016, 6.9570, 6.0039, 4.5742, 4.2500, 9.5547, 9.1172, 0.2153, 5.5117, 0.2257, 7.5117, 5.9922, 1.3135, 9.8359, 8.6328, 5.5547, 1.7725, 8.4766, 1.6074, 2.9023, 2.7344, 9.3906, 9.8516, 6.4180, 2.9727, 3.4805, 2.5078, 1.4434, 1.4043, 8.1719, 1.0029, 3.3887, 5.6641, 4.2070, 0.0256, 8.5859, 8.1406, 6.2148, 4.1445, 6.7344, 8.5469, 9.3516, 2.2891, 2.2598, 0.4744, 3.6973, 3.3398, 4.1406, 6.8008, 8.1562, 0.3940, 4.7305, 7.7227, 0.9595, 3.7910, 5.9492, 4.8164, 5.1211, 8.1719, 1.5664, 7.3086, 9.0469, 7.7891, 3.5703, 0.5425, 4.6758, 4.0312, 0.3860, 6.8516, 8.4219, 6.6797, 9.9219, 4.3438, 7.5000, 9.5938, ]\n",
      "[3.4395, 5.6445, 8.5156, 3.8320, 5.5547, 7.7852, 8.4219, 6.9258, 5.9883, 4.9258, 2.0430, 3.4824, 2.2871, 4.6523, 1.1562, 7.4922, 8.2422, 3.0000, 1.5615, 8.5859, 6.7891, 6.1953, 2.9961, 6.7305, 5.8242, 9.6484, 3.8125, 4.2148, 7.0234, 6.6523, 1.8174, 4.2266, 9.2031, 7.9844, 5.7461, 0.3494, 7.1328, 2.4707, 1.8721, 2.9238, 5.4805, 6.4062, 2.6582, 7.6094, 5.7227, 2.5137, 3.2676, 5.1484, 8.2734, 5.6289, 9.5234, 6.3516, 2.3359, 2.2480, 9.3984, 6.6406, 4.0000, 4.9688, 1.6787, 4.2266, 6.6094, 6.3164, 5.8242, 6.4336, 4.5430, 4.4922, 3.2656, 2.4902, 5.3281, 5.8906, 5.4844, 7.0820, 0.5469, 1.2168, 8.9844, 7.8203, 3.8574, 2.3301, 5.6953, 8.2891, 5.9570, 4.1406, 6.2656, 2.6250, 4.1367, 6.5195, 7.5508, 6.6953, 2.7422, 5.3203, 7.2188, 2.1328, 3.4102, 6.2070, 4.5234, 1.3154, 3.5449, 3.4688, 7.5859, 6.2930, 2.3320, 4.6445, 9.3828, 1.2979, 4.8281, 3.2148, 2.2637, 0.7944, 7.6836, 7.3438, 4.4531, 2.9395, 5.7773, 3.7012, 3.6738, 3.1895, 2.2715, 5.6289, 1.2979, 7.1211, 8.3516, 1.7266, 8.6875, 2.7031, 6.9570, 7.6758, 2.1992, 9.1172, ]\n",
      "[0.7861, 4.6562, 2.3203, 9.2891, 2.8105, 7.8281, 8.3984, 1.6152, 3.0508, 2.2676, 0.2000, 9.6953, 7.8906, 7.5977, 5.2031, 9.2500, 1.0635, 7.8672, 3.1660, 2.9727, 8.7422, 8.7422, 6.6328, 4.9531, 1.1055, 9.3125, 4.3945, 5.2695, 0.2415, 1.0684, 8.6250, 1.4160, 6.9570, 9.1328, 6.2578, 0.0890, 7.2578, 3.1367, 6.2148, 6.3711, 4.6914, 7.8945, 4.4727, 8.8594, 4.7578, 0.4551, 9.6719, 8.4141, 8.6406, 1.9883, 8.1562, 2.1914, 9.4375, 9.1094, 6.9570, 6.0078, 4.5742, 4.2500, 9.5625, 9.1250, 0.2098, 5.5117, 0.2205, 7.5156, 5.9961, 1.3105, 9.8438, 8.6406, 5.5547, 1.7686, 8.4844, 1.6035, 2.9023, 2.7363, 9.3906, 9.8516, 6.4219, 2.9727, 3.4805, 2.5020, 1.4395, 1.4023, 8.1719, 1.0010, 3.3887, 5.6641, 4.2031, 0.0190, 8.5938, 8.1406, 6.2148, 4.1484, 6.7383, 8.5469, 9.3594, 2.2891, 2.2578, 0.4714, 3.6953, 3.3379, 4.1445, 6.8047, 8.1562, 0.3928, 4.7305, 7.7266, 0.9580, 3.7930, 5.9453, 4.8164, 5.1211, 8.1797, 1.5635, 7.3125, 9.0547, 7.7969, 3.5723, 0.5371, 4.6797, 4.0312, 0.3787, 6.8594, 8.4219, 6.6836, 9.9219, 4.3398, 7.5078, 9.5938, ]\n",
      "[0.7861, 4.6562, 2.3203, 9.2891, 2.8105, 7.8281, 8.3984, 1.6152, 3.0508, 2.2676, 0.2000, 9.6953, 7.8906, 7.5977, 5.2031, 9.2500, 1.0635, 7.8672, 3.1660, 2.9727, 8.7422, 8.7422, 6.6328, 4.9531, 1.1055, 9.3125, 4.3945, 5.2695, 0.2415, 1.0684, 8.6250, 1.4160, 6.9570, 9.1328, 6.2578, 0.0890, 7.2578, 3.1367, 6.2148, 6.3711, 4.6914, 7.8945, 4.4727, 8.8594, 4.7578, 0.4551, 9.6719, 8.4141, 8.6406, 1.9883, 8.1562, 2.1914, 9.4375, 9.1094, 6.9570, 6.0078, 4.5742, 4.2500, 9.5625, 9.1250, 0.2098, 5.5117, 0.2205, 7.5156, 5.9961, 1.3105, 9.8438, 8.6406, 5.5547, 1.7686, 8.4844, 1.6035, 2.9023, 2.7363, 9.3906, 9.8516, 6.4219, 2.9727, 3.4805, 2.5020, 1.4395, 1.4023, 8.1719, 1.0010, 3.3887, 5.6641, 4.2031, 0.0190, 8.5938, 8.1406, 6.2148, 4.1484, 6.7383, 8.5469, 9.3594, 2.2891, 2.2578, 0.4714, 3.6953, 3.3379, 4.1445, 6.8047, 8.1562, 0.3928, 4.7305, 7.7266, 0.9580, 3.7930, 5.9453, 4.8164, 5.1211, 8.1797, 1.5635, 7.3125, 9.0547, 7.7969, 3.5723, 0.5371, 4.6797, 4.0312, 0.3787, 6.8594, 8.4219, 6.6836, 9.9219, 4.3398, 7.5078, 9.5938, ]\n",
      "[0.7861, 4.6562, 2.3203, 9.2891, 2.8105, 7.8281, 8.3984, 1.6152, 3.0508, 2.2676, 0.2001, 9.6953, 7.8906, 7.5977, 5.2031, 9.2500, 1.0645, 7.8672, 3.1660, 2.9727, 8.7422, 8.7422, 6.6328, 4.9531, 1.1055, 9.3125, 4.3945, 5.2695, 0.2419, 1.0684, 8.6250, 1.4160, 6.9570, 9.1328, 6.2578, 0.0890, 7.2578, 3.1367, 6.2148, 6.3711, 4.6914, 7.8945, 4.4727, 8.8594, 4.7578, 0.4553, 9.6719, 8.4141, 8.6406, 1.9883, 8.1562, 2.1914, 9.4375, 9.1094, 6.9570, 6.0078, 4.5742, 4.2500, 9.5625, 9.1250, 0.2103, 5.5117, 0.2208, 7.5156, 5.9961, 1.3105, 9.8438, 8.6406, 5.5547, 1.7686, 8.4844, 1.6035, 2.9023, 2.7363, 9.3906, 9.8516, 6.4219, 2.9727, 3.4805, 2.5020, 1.4395, 1.4023, 8.1719, 1.0010, 3.3887, 5.6641, 4.2031, 0.0194, 8.5938, 8.1406, 6.2148, 4.1484, 6.7383, 8.5469, 9.3594, 2.2891, 2.2578, 0.4717, 3.6953, 3.3379, 4.1445, 6.8047, 8.1562, 0.3928, 4.7305, 7.7266, 0.9580, 3.7930, 5.9453, 4.8164, 5.1211, 8.1797, 1.5635, 7.3125, 9.0547, 7.7969, 3.5723, 0.5376, 4.6797, 4.0312, 0.3792, 6.8594, 8.4219, 6.6836, 9.9219, 4.3398, 7.5078, 9.5938, ]\n",
      "[0.7861, 4.6562, 2.3203, 9.2891, 2.8105, 7.8281, 8.3984, 1.6152, 3.0508, 2.2676, 0.2000, 9.6953, 7.8906, 7.5977, 5.2031, 9.2500, 1.0635, 7.8672, 3.1660, 2.9727, 8.7422, 8.7422, 6.6328, 4.9531, 1.1055, 9.3125, 4.3945, 5.2695, 0.2416, 1.0684, 8.6250, 1.4160, 6.9570, 9.1328, 6.2578, 0.0890, 7.2578, 3.1367, 6.2148, 6.3711, 4.6914, 7.8945, 4.4727, 8.8594, 4.7578, 0.4551, 9.6719, 8.4141, 8.6406, 1.9883, 8.1562, 2.1914, 9.4375, 9.1094, 6.9570, 6.0078, 4.5742, 4.2500, 9.5625, 9.1250, 0.2098, 5.5117, 0.2205, 7.5156, 5.9961, 1.3105, 9.8438, 8.6406, 5.5547, 1.7686, 8.4844, 1.6035, 2.9023, 2.7363, 9.3906, 9.8516, 6.4219, 2.9727, 3.4805, 2.5020, 1.4395, 1.4023, 8.1719, 1.0010, 3.3887, 5.6641, 4.2031, 0.0190, 8.5938, 8.1406, 6.2148, 4.1484, 6.7383, 8.5469, 9.3594, 2.2891, 2.2578, 0.4714, 3.6953, 3.3379, 4.1445, 6.8047, 8.1562, 0.3928, 4.7305, 7.7266, 0.9580, 3.7930, 5.9453, 4.8164, 5.1211, 8.1797, 1.5635, 7.3125, 9.0547, 7.7969, 3.5723, 0.5371, 4.6797, 4.0312, 0.3787, 6.8594, 8.4219, 6.6836, 9.9219, 4.3398, 7.5078, 9.5938, ]]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0.7861, 4.6562, 2.3203,  ..., 4.3398, 7.5078, 9.5938],\n",
       "         [0.7861, 4.6562, 2.3203,  ..., 4.3398, 7.5078, 9.5938],\n",
       "         [0.7891, 4.6562, 2.3262,  ..., 4.3438, 7.5039, 9.5938],\n",
       "         ...,\n",
       "         [0.7861, 4.6562, 2.3203,  ..., 4.3398, 7.5078, 9.5938],\n",
       "         [0.7861, 4.6562, 2.3203,  ..., 4.3398, 7.5078, 9.5938],\n",
       "         [0.7861, 4.6562, 2.3203,  ..., 4.3398, 7.5078, 9.5938]],\n",
       "        device='cuda:0', dtype=torch.float16),\n",
       " tensor([[0.7861, 4.6562, 2.3203,  ..., 4.3398, 7.5078, 9.5938],\n",
       "         [0.7861, 4.6562, 2.3203,  ..., 4.3398, 7.5078, 9.5938],\n",
       "         [0.7886, 4.6562, 2.3262,  ..., 4.3438, 7.5000, 9.5938],\n",
       "         ...,\n",
       "         [0.7861, 4.6562, 2.3203,  ..., 4.3398, 7.5078, 9.5938],\n",
       "         [0.7861, 4.6562, 2.3203,  ..., 4.3398, 7.5078, 9.5938],\n",
       "         [0.7861, 4.6562, 2.3203,  ..., 4.3398, 7.5078, 9.5938]],\n",
       "        device='cuda:0', dtype=torch.float16))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.isclose(dumb_attn(Q, K, V), flashattn(Q, K, V))\n",
    "# dumb_attn(Q, K, V), flashattn(Q, K, V)\n",
    "\n",
    "# F.scaled_dot_product_attention(Q, K, V, scale=1), flashattn(Q, K, V)\n",
    "# module.flash_attn(Q, K, V)\n",
    "r1 = dumb_attn(Q, K, V)\n",
    "r2 = module.flash_attn(Q, K, V)\n",
    "(r1-r2).abs().mean()\n",
    "r1, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([4., 4., 3., 1., 5.]),\n",
       " tensor([4., 4., 3., 1., 5., 0., 0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randint(1, 9, (5,)).float()\n",
    "b = torch.hstack((a, torch.zeros((5,)).float()))\n",
    "a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.1947, 0.1947, 0.0716, 0.0097, 0.5293]),\n",
       " tensor([0.1913, 0.1913, 0.0704, 0.0095, 0.5200, 0.0035, 0.0035, 0.0035, 0.0035,\n",
       "         0.0035]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.softmax(0), b.softmax(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    0.0000,     0.0000,     0.0183,     1.0000])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([65.000000, 42.000000, 80.000000, 84.000000], dtype=torch.float32)\n",
    "(a - a.max()).exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cudaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
