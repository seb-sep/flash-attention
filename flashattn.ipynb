{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure to use the right version of pytorch when testing on nvidia/amd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "torch.set_printoptions(sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "M = 32\n",
    "\n",
    "\n",
    "def flashattn(Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor) -> torch.Tensor:\n",
    "    assert Q.shape == K.shape == V.shape\n",
    "    assert len(Q.shape) == 2\n",
    "\n",
    "    K = K.T\n",
    "\n",
    "    # seq length, inner dim of representations\n",
    "    N, d = Q.shape\n",
    "\n",
    "    # of kv vectors per tile\n",
    "    bc = math.ceil(M/(4*d))\n",
    "    tc = math.ceil(N/bc)\n",
    "    K_shared = torch.empty((d, bc), dtype=Q.dtype)\n",
    "    V_shared = torch.empty((bc, d), dtype=Q.dtype)\n",
    "\n",
    "    # of q vectors per tile\n",
    "    br = min(math.ceil(M/(4*d)), d)\n",
    "    tr = math.ceil(N/br)\n",
    "    Q_shared = torch.empty((br, d), dtype=Q.dtype)\n",
    "\n",
    "    # print(f'bc={bc}, br={br}')\n",
    "    # print(f'tc={tc}, tr={tr}')\n",
    "\n",
    "    # output matrix\n",
    "    O = torch.zeros_like(Q)\n",
    "    O_shared = torch.empty_like(Q_shared)\n",
    "\n",
    "    # intermediate rowmaxes\n",
    "    m = torch.full((N,), -torch.inf, dtype=Q.dtype)\n",
    "    m_shared = torch.empty((br, 1), dtype=Q.dtype)\n",
    "    # intermediate normalization constants\n",
    "    l = torch.full((N,), 0, dtype=Q.dtype)\n",
    "    l_shared = torch.empty((br, 1), dtype=Q.dtype)\n",
    "\n",
    "    for i in range(tc):\n",
    "        # load k, v chunks\n",
    "        # make sure we load in k as its transposed version\n",
    "        K_shared[:, :] = K[:, i*bc:(i+1)*bc]\n",
    "        V_shared[:, :] = V[i*bc:(i+1)*bc, :]\n",
    "\n",
    "        for j in range(tr):\n",
    "            # load in q, o, m, l\n",
    "            Q_shared[:, :] = Q[j*br:(j+1)*br, :]\n",
    "            # if i == 0: print(f'Q: {Q_shared}')\n",
    "            O_shared[:, :] = O[j*br:(j+1)*br, :]\n",
    "            m_shared[:, :] = m[j*br:(j+1)*br].unsqueeze(-1)\n",
    "            l_shared[:, :] = l[j*br:(j+1)*br].unsqueeze(-1)\n",
    "            \n",
    "            S = Q_shared @ K_shared\n",
    "\n",
    "            # get row-wise softmax statistics\n",
    "            mt = S.max(dim=1).values.reshape(-1, 1)\n",
    "\n",
    "            Pt = torch.exp(S - mt)\n",
    "            lt = Pt.sum(dim=1).reshape(-1, 1)\n",
    "\n",
    "            # compute new statistics\n",
    "            m_new = torch.max(mt, m_shared)\n",
    "            l_new = (torch.exp(m_shared - m_new) * l_shared) + (torch.exp(mt - m_new) * lt)\n",
    "\n",
    "\n",
    "            # update chunk of output\n",
    "            O_new = (l_shared * torch.exp(m_shared - m_new) * O_shared + torch.exp(mt - m_new) * Pt @ V_shared) / l_new\n",
    "            O[j*br:(j+1)*br, :] = O_new\n",
    "            \n",
    "            m[j*br:(j+1)*br] = m_new.flatten()\n",
    "            l[j*br:(j+1)*br] = l_new.flatten()\n",
    "\n",
    "    return O\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.9692, 0.8667, 0.2769,  ..., 0.4790, 0.0756, 0.5806],\n",
       "         [0.3923, 0.5615, 0.7637,  ..., 0.9009, 0.4824, 0.7891],\n",
       "         [0.6646, 0.3252, 0.7012,  ..., 0.2593, 0.7358, 0.0787],\n",
       "         ...,\n",
       "         [0.1360, 0.9292, 0.6895,  ..., 0.6885, 0.7935, 0.1575],\n",
       "         [0.6240, 0.0032, 0.4893,  ..., 0.9106, 0.8154, 0.3074],\n",
       "         [0.5620, 0.2218, 0.2180,  ..., 0.9136, 0.2301, 0.7876]],\n",
       "        device='cuda:0', dtype=torch.float16),\n",
       " tensor([[0.0101, 0.1547, 0.5068,  ..., 0.3699, 0.1704, 0.1504],\n",
       "         [0.0531, 0.0361, 0.3989,  ..., 0.4812, 0.9629, 0.5723],\n",
       "         [0.8521, 0.2830, 0.4976,  ..., 0.5796, 0.7998, 0.8433],\n",
       "         ...,\n",
       "         [0.7920, 0.8970, 0.3716,  ..., 0.8301, 0.1196, 0.1761],\n",
       "         [0.6099, 0.3625, 0.9199,  ..., 0.4976, 0.0604, 0.6001],\n",
       "         [0.1112, 0.6943, 0.0289,  ..., 0.2869, 0.7617, 0.9707]],\n",
       "        device='cuda:0', dtype=torch.float16),\n",
       " tensor([[0.5015, 0.7148, 0.1860,  ..., 0.1707, 0.5869, 0.5840],\n",
       "         [0.9722, 0.1265, 0.3621,  ..., 0.8672, 0.4062, 0.6899],\n",
       "         [0.2426, 0.8096, 0.4224,  ..., 0.8320, 0.6343, 0.5591],\n",
       "         ...,\n",
       "         [0.2800, 0.2991, 0.3032,  ..., 0.5005, 0.6636, 0.2101],\n",
       "         [0.3945, 0.4426, 0.5894,  ..., 0.3760, 0.8350, 0.2983],\n",
       "         [0.6035, 0.0748, 0.9316,  ..., 0.1718, 0.3318, 0.5723]],\n",
       "        device='cuda:0', dtype=torch.float16))"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import sqrt\n",
    "def dumb_attn(Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"equivalent to F.scaled_dot_product_attention(Q, K, V, scale=1)\"\"\"\n",
    "    d = Q.shape[1]\n",
    "    return torch.softmax((Q)@ K.T, dim=1) @ V\n",
    "    # return F.scaled_dot_product_attention(Q, K, V, scale=1)\n",
    "\n",
    "\n",
    "# have to support up to like d=4096\n",
    "# we're giving up and only supporting fp16 (like flashattn)\n",
    "Q = torch.rand((64, 256), dtype=torch.float16, device='cuda') \n",
    "K = torch.rand((64, 256), dtype=torch.float16, device='cuda') \n",
    "V = torch.rand((64, 256), dtype=torch.float16, device='cuda') \n",
    "\n",
    "# K[-1] = torch.zeros((2,))\n",
    "# V[-1] = torch.zeros((2,))\n",
    "\n",
    "\n",
    "# Q = torch.randint(1, 9, (8, 4), device='cuda').to(torch.float16)\n",
    "# K = torch.randint(1, 9, (8, 4), device='cuda').to(torch.float16)\n",
    "# V = torch.randint(1, 9, (8, 4), device='cuda').to(torch.float16)\n",
    "\n",
    "# Q = torch.tensor([[0., 1.],\n",
    "#          [2., 3.]])\n",
    "\n",
    "# K = torch.tensor([[0., 1.5],\n",
    "#          [2., 3.]])\n",
    "\n",
    "# V = torch.tensor([[1., 0.],\n",
    "#          [0., 1.]])\n",
    "Q, K, V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/seb/Code/flash-attention/rocm/flash_attention.hip -> /home/seb/Code/flash-attention/rocm/flash_attention_hip.hip [ok]\n",
      "Total number of unsupported CUDA function calls: 0\n",
      "\n",
      "\n",
      "Total number of replaced kernel launches: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The input conditions for extension module m have changed. Bumping to version 20 and re-building as m_v20...\n",
      "\u001b[92mSuccessfully preprocessed all matching files.\u001b[0m\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file build/build.ninja...\n",
      "Building extension module m_v20...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/2] /opt/rocm-6.1.3/bin/hipcc  -DWITH_HIP -DTORCH_EXTENSION_NAME=m_v20 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/seb/Code/pyenvs/rocmenv/lib/python3.12/site-packages/torch/include -isystem /home/seb/Code/pyenvs/rocmenv/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -isystem /home/seb/Code/pyenvs/rocmenv/lib/python3.12/site-packages/torch/include/TH -isystem /home/seb/Code/pyenvs/rocmenv/lib/python3.12/site-packages/torch/include/THC -isystem /home/seb/Code/pyenvs/rocmenv/lib/python3.12/site-packages/torch/include/THH -isystem /opt/rocm-6.1.3/include -isystem /home/seb/miniconda3/include/python3.12 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 --offload-arch=\"gfx1100\" -fPIC -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -DCUDA_HAS_FP16=1 -D__HIP_NO_HALF_OPERATORS__=1 -D__HIP_NO_HALF_CONVERSIONS__=1 -fno-gpu-rdc -c /home/seb/Code/flash-attention/rocm/flash_attention_hip.hip -o flash_attention_hip.cuda.o \n",
      "/home/seb/Code/flash-attention/rocm/flash_attention_hip.hip:329:5: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  329 |     hipGetDeviceProperties(&props, 0);\n",
      "      |     ^~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~~\n",
      "/opt/rocm-6.1.3/include/hip/hip_runtime_api.h:91:32: note: expanded from macro 'hipGetDeviceProperties'\n",
      "   91 | #define hipGetDeviceProperties hipGetDevicePropertiesR0600\n",
      "      |                                ^~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/home/seb/Code/flash-attention/rocm/flash_attention_hip.hip:361:30: warning: format specifies type 'int' but the argument has type 'size_t' (aka 'unsigned long') [-Wformat]\n",
      "  361 |     printf(\"tc=%d, tr=%d\\n\", tc, tr);\n",
      "      |                ~~            ^~\n",
      "      |                %zu\n",
      "/home/seb/Code/flash-attention/rocm/flash_attention_hip.hip:361:34: warning: format specifies type 'int' but the argument has type 'size_t' (aka 'unsigned long') [-Wformat]\n",
      "  361 |     printf(\"tc=%d, tr=%d\\n\", tc, tr);\n",
      "      |                       ~~         ^~\n",
      "      |                       %zu\n",
      "3 warnings generated when compiling for gfx1100.\n",
      "/home/seb/Code/flash-attention/rocm/flash_attention_hip.hip:329:5: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  329 |     hipGetDeviceProperties(&props, 0);\n",
      "      |     ^~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~~\n",
      "/opt/rocm-6.1.3/include/hip/hip_runtime_api.h:91:32: note: expanded from macro 'hipGetDeviceProperties'\n",
      "   91 | #define hipGetDeviceProperties hipGetDevicePropertiesR0600\n",
      "      |                                ^~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/home/seb/Code/flash-attention/rocm/flash_attention_hip.hip:361:30: warning: format specifies type 'int' but the argument has type 'size_t' (aka 'unsigned long') [-Wformat]\n",
      "  361 |     printf(\"tc=%d, tr=%d\\n\", tc, tr);\n",
      "      |                ~~            ^~\n",
      "      |                %zu\n",
      "/home/seb/Code/flash-attention/rocm/flash_attention_hip.hip:361:34: warning: format specifies type 'int' but the argument has type 'size_t' (aka 'unsigned long') [-Wformat]\n",
      "  361 |     printf(\"tc=%d, tr=%d\\n\", tc, tr);\n",
      "      |                       ~~         ^~\n",
      "      |                       %zu\n",
      "3 warnings generated when compiling for host.\n",
      "[2/2] c++ flash_attention_hip.cuda.o -shared -L/home/seb/Code/pyenvs/rocmenv/lib/python3.12/site-packages/torch/lib -lc10 -lc10_hip -ltorch_cpu -ltorch_hip -ltorch -ltorch_python -L/opt/rocm-6.1.3/lib -lamdhip64 -o m_v20.so\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module m_v20...\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.cpp_extension import load\n",
    "module = load(\n",
    "    name='m',\n",
    "    # sources=['cuda/main.cpp', 'cuda/flash_attention.cu'],\n",
    "    sources=['rocm/flash_attention.hip',],\n",
    "    extra_cflags=['--offload-arch=\"gfx1100\"',],\n",
    "    build_directory='build',\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tc=8, tr=8\n",
      "Q:\n",
      "[[0.2263, 0.8062, 0.5610, 0.5864, 0.2240, 0.1256, 0.2905, 0.5786, 0.0806, 0.0186, 0.0041, 0.9883, 0.2656, 0.7109, 0.4702, 0.1790, 0.3147, 0.4700, 0.8726, 0.0129, 0.1383, 0.8765, 0.9248, 0.5820, 0.0030, 0.6089, 0.0879, 0.9785, 0.0206, 0.9170, 0.2590, 0.9111, 0.4058, 0.0702, 0.0619, 0.7021, 0.7764, 0.7290, 0.9497, 0.6519, 0.8887, 0.9346, 0.1448, 0.8726, 0.4983, 0.9072, 0.9863, 0.3220, 0.9624, 0.1345, 0.3250, 0.7100, 0.0387, 0.8130, 0.5396, 0.1399, 0.3662, 0.5425, 0.1506, 0.3972, 0.5220, 0.3684, 0.8857, 0.1910, 0.3687, 0.4619, 0.2306, 0.3833, 0.1346, 0.3931, 0.8198, 0.4258, 0.8188, 0.4353, 0.8340, 0.5137, 0.7925, 0.2195, 0.1945, 0.7764, 0.4858, 0.5781, 0.6514, 0.0022, 0.3240, 0.1368, 0.5532, 0.5005, 0.7114, 0.5161, 0.8296, 0.6011, 0.7104, 0.7715, 0.4373, 0.7983, 0.2413, 0.4258, 0.8740, 0.8105, 0.9375, 0.5332, 0.4858, 0.9829, 0.3726, 0.4617, 0.3958, 0.3335, 0.3958, 0.9741, 0.1256, 0.1310, 0.0270, 0.3623, 0.9990, 0.2546, 0.6738, 0.9146, 0.4150, 0.3020, 0.3286, 0.1879, 0.8955, 0.1843, 0.1185, 0.4988, 0.6895, 0.4102, 0.6724, 0.5088, 0.4158, 0.4292, 0.7134, 0.7163, 0.8271, 0.2776, 0.4219, 0.5615, 0.6006, 0.3457, 0.9502, 0.6953, 0.7505, 0.1566, 0.0776, 0.7661, 0.8428, 0.9014, 0.2512, 0.9497, 0.2382, 0.3945, 0.0779, 0.1758, 0.0521, 0.5864, 0.5615, 0.8506, 0.8267, 0.8237, 0.1960, 0.0217, 0.2177, 0.8613, 0.3843, 0.1586, 0.4412, 0.9575, 0.8945, 0.5737, 0.5894, 0.1111, 0.4543, 0.1392, 0.4058, 0.8726, 0.1116, 0.7402, 0.1041, 0.2324, 0.1849, 0.0613, 0.4851, 0.1677, 0.2325, 0.8135, 0.5547, 0.4656, 0.8862, 0.7354, 0.2866, 0.1190, 0.4790, 0.3813, 0.4595, 0.8213, 0.1860, 0.3357, 0.8589, 0.2354, 0.4626, 0.5439, 0.5659, 0.1981, 0.4727, 0.0995, 0.6797, 0.4661, 0.0677, 0.9731, 0.6274, 0.8853, 0.5371, 0.4167, 0.7124, 0.0585, 0.6987, 0.0218, 0.0067, 0.6211, 0.5620, 0.1145, 0.7764, 0.8096, 0.8608, 0.1470, 0.1467, 0.3533, 0.8867, 0.3840, 0.7510, 0.1758, 0.8984, 0.2443, 0.0125, 0.0388, 0.6079, 0.1549, 0.4443, 0.4023, 0.1265, 0.5698, 0.9985, 0.2310, 0.2756, 0.3782, 0.8462, 0.1575, 0.3555, 0.2793, 0.2998, 0.2891, 0.2756, 0.3640, 0.2391, 0.7061, ]\n",
      "[0.5781, 0.7822, 0.6079, 0.2830, 0.3069, 0.6465, 0.7827, 0.7930, 0.1782, 0.8062, 0.6978, 0.7559, 0.8975, 0.5591, 0.4412, 0.5732, 0.8311, 0.6851, 0.1113, 0.5117, 0.1902, 0.3904, 0.3210, 0.1531, 0.3623, 0.9766, 0.2603, 0.7241, 0.8994, 0.0413, 0.8359, 0.0386, 0.3752, 0.6553, 0.8589, 0.2380, 0.8506, 0.6689, 0.7842, 0.5415, 0.7588, 0.9277, 0.5518, 0.1510, 0.1156, 0.2257, 0.1682, 0.3765, 0.1467, 0.8887, 0.8042, 0.6089, 0.5527, 0.4990, 0.5391, 0.5151, 0.1216, 0.4875, 0.3569, 0.6714, 0.5889, 0.8364, 0.0110, 0.0646, 0.2534, 0.1118, 0.0760, 0.3936, 0.1600, 0.4443, 0.8794, 0.0956, 0.0859, 0.3831, 0.4353, 0.7139, 0.5522, 0.2206, 0.0363, 0.3069, 0.4673, 0.6357, 0.9985, 0.1316, 0.4216, 0.4443, 0.6914, 0.7783, 0.3508, 0.4924, 0.7334, 0.7275, 0.4111, 0.7544, 0.4219, 0.0483, 0.2725, 0.5933, 0.1532, 0.1570, 0.4788, 0.7593, 0.5171, 0.8003, 0.7866, 0.3374, 0.6748, 0.7646, 0.0832, 0.7324, 0.7007, 0.2323, 0.4927, 0.0388, 0.0353, 0.7051, 0.6250, 0.1857, 0.8433, 0.0892, 0.0523, 0.2590, 0.4309, 0.1848, 0.2013, 0.6753, 0.0005, 0.0282, 0.3340, 0.8340, 0.2227, 0.5356, 0.8633, 0.1790, 0.4177, 0.1117, 0.8804, 0.4502, 0.4258, 0.8452, 0.8979, 0.3047, 0.7275, 0.6353, 0.7827, 0.9097, 0.8999, 0.3989, 0.6421, 0.4390, 0.5718, 0.2844, 0.1498, 0.9990, 0.6250, 0.8618, 0.3906, 0.8472, 0.1259, 0.8232, 0.4353, 0.3184, 0.7461, 0.5757, 0.0525, 0.8052, 0.6768, 0.2534, 0.3560, 0.7661, 0.1287, 0.9829, 0.4854, 0.9961, 0.1752, 0.0297, 0.5903, 0.9658, 0.3313, 0.4773, 0.9429, 0.9023, 0.1938, 0.1106, 0.9067, 0.2661, 0.3750, 0.0175, 0.6113, 0.2218, 0.4189, 0.2661, 0.1760, 0.1351, 0.7295, 0.7651, 0.9775, 0.0027, 0.7368, 0.6802, 0.4866, 0.1597, 0.4336, 0.0361, 0.3586, 0.1334, 0.6104, 0.8750, 0.7944, 0.4094, 0.8716, 0.7002, 0.7476, 0.9292, 0.1827, 0.0619, 0.5962, 0.1287, 0.8408, 0.0661, 0.6733, 0.7910, 0.9443, 0.2478, 0.3958, 0.0595, 0.4177, 0.7842, 0.9644, 0.1078, 0.2296, 0.9111, 0.2305, 0.5679, 0.9160, 0.1573, 0.9312, 0.9946, 0.3196, 0.4277, 0.7295, 0.1370, 0.7617, 0.4119, 0.5254, 0.3630, 0.4797, 0.8447, 0.6973, 0.3291, 0.0823, 0.1136, 0.4287, 0.2235, 0.6030, 0.8633, ]\n",
      "[0.6396, 0.8936, 0.9707, 0.0131, 0.2096, 0.4587, 0.5933, 0.3796, 0.6138, 0.7500, 0.8062, 0.4912, 0.2257, 0.5259, 0.6758, 0.1663, 0.2676, 0.0013, 0.9976, 0.6572, 0.5674, 0.0676, 0.6802, 0.8330, 0.7251, 0.1638, 0.1724, 0.2998, 0.2009, 0.0013, 0.9292, 0.3311, 0.3179, 0.3669, 0.8726, 0.8931, 0.5151, 0.7949, 0.2893, 0.1042, 0.2537, 0.4932, 0.6235, 0.4690, 0.1255, 0.5400, 0.7524, 0.6299, 0.3040, 0.5063, 0.0835, 0.8462, 0.4177, 0.5864, 0.1836, 0.4189, 0.4509, 0.7725, 0.4304, 0.0651, 0.5386, 0.9854, 0.8350, 0.1339, 0.3684, 0.9614, 0.7280, 0.1760, 0.6055, 0.0779, 0.3896, 0.3594, 0.0353, 0.3628, 0.1312, 0.1825, 0.9912, 0.7168, 0.7266, 0.1384, 0.6558, 0.5693, 0.9619, 0.0818, 0.0026, 0.5347, 0.7881, 0.1874, 0.6650, 0.2522, 0.7241, 0.5957, 0.0945, 0.2339, 0.4507, 0.0996, 0.7812, 0.3787, 0.4910, 0.2286, 0.4880, 0.7275, 0.3188, 0.3479, 0.5767, 0.1158, 0.0688, 0.2573, 0.1554, 0.7690, 0.5845, 0.0784, 0.7593, 0.7222, 0.2810, 0.3999, 0.2198, 0.6616, 0.1504, 0.5527, 0.4758, 0.0608, 0.7964, 0.1766, 0.8511, 0.1400, 0.5913, 0.4175, 0.7944, 0.1733, 0.5352, 0.1180, 0.4526, 0.6416, 0.2698, 0.8340, 0.5127, 0.8252, 0.0002, 0.2412, 0.5981, 0.2927, 0.9224, 0.2808, 0.1483, 0.7329, 0.1643, 0.1848, 0.0376, 0.0864, 0.2296, 0.3477, 0.3389, 0.2864, 0.4878, 0.2325, 0.4409, 0.1036, 0.5283, 0.0021, 0.3618, 0.1110, 0.8838, 0.4700, 0.0884, 0.4089, 0.8389, 0.1915, 0.3384, 0.4653, 0.7783, 0.2019, 0.3923, 0.8945, 0.0045, 0.9507, 0.0879, 0.4983, 0.5879, 0.6333, 0.8555, 0.5640, 0.6699, 0.7549, 0.8945, 0.9541, 0.8828, 0.5005, 0.5371, 0.6406, 0.8018, 0.3777, 0.2686, 0.6812, 0.0579, 0.3115, 0.5547, 0.6836, 0.2578, 0.6353, 0.2866, 0.3770, 0.7939, 0.4712, 0.2520, 0.8936, 0.8467, 0.5195, 0.7461, 0.1970, 0.2207, 0.0479, 0.5762, 0.2720, 0.0184, 0.5508, 0.3477, 0.6792, 0.6885, 0.9751, 0.3130, 0.4912, 0.4731, 0.4834, 0.0577, 0.0321, 0.0021, 0.3726, 0.5181, 0.6577, 0.5483, 0.7368, 0.0551, 0.9658, 0.0636, 0.1411, 0.7964, 0.6221, 0.4102, 0.5122, 0.5562, 0.0999, 0.0112, 0.4177, 0.8408, 0.0867, 0.4407, 0.3525, 0.6426, 0.0103, 0.9478, 0.9927, 0.6763, 0.0780, 0.8940, 0.0098, ]\n",
      "[0.1001, 0.4592, 0.1037, 0.8740, 0.4583, 0.9126, 0.3369, 0.5728, 0.6860, 0.1232, 0.8823, 0.8208, 0.5581, 0.7280, 0.4688, 0.2534, 0.7954, 0.8662, 0.9253, 0.8638, 0.7778, 0.5225, 0.0656, 0.0043, 0.5195, 0.1388, 0.2163, 0.1661, 0.9604, 0.2976, 0.8784, 0.3989, 0.0958, 0.2566, 0.0131, 0.3853, 0.0798, 0.6484, 0.4673, 0.6841, 0.0117, 0.5488, 0.0457, 0.6978, 0.5913, 0.4514, 0.0884, 0.9482, 0.4421, 0.0255, 0.4226, 0.4797, 0.6626, 0.1172, 0.7480, 0.7388, 0.7671, 0.8828, 0.9585, 0.5576, 0.5610, 0.3965, 0.5923, 0.4182, 0.8628, 0.0836, 0.9106, 0.5737, 0.9668, 0.3721, 0.2671, 0.9863, 0.0162, 0.0247, 0.1013, 0.1827, 0.5591, 0.8828, 0.5762, 0.5586, 0.8599, 0.2328, 0.4573, 0.9663, 0.7783, 0.4846, 0.2021, 0.9141, 0.2236, 0.5962, 0.5161, 0.7500, 0.5786, 0.4434, 0.9287, 0.1092, 0.3381, 0.5620, 0.5044, 0.8013, 0.4250, 0.5508, 0.2058, 0.4104, 0.2654, 0.4985, 0.1780, 0.5654, 0.6328, 0.7451, 0.1787, 0.2009, 0.1470, 0.9683, 0.7856, 0.9604, 0.9707, 0.7368, 0.1655, 0.2673, 0.0105, 0.1281, 0.3831, 0.7393, 0.6807, 0.4668, 0.0242, 0.3540, 0.2452, 0.8398, 0.6016, 0.7642, 0.5142, 0.7173, 0.3962, 0.9004, 0.3789, 0.6406, 0.7666, 0.2568, 0.9355, 0.0904, 0.9487, 0.8604, 0.5488, 0.5732, 0.4834, 0.5811, 0.2275, 0.2373, 0.0430, 0.7534, 0.3992, 0.1157, 0.1918, 0.2323, 0.7437, 0.8159, 0.3086, 0.6963, 0.1527, 0.5996, 0.7827, 0.6650, 0.1357, 0.2029, 0.1957, 0.8730, 0.5083, 0.9932, 0.8052, 0.1788, 0.0107, 0.3066, 0.8413, 0.3960, 0.2253, 0.3911, 0.5186, 0.3750, 0.8750, 0.8047, 0.6367, 0.0888, 0.1331, 0.5454, 0.7144, 0.3503, 0.9858, 0.8843, 0.2566, 0.4746, 0.7002, 0.2976, 0.5776, 0.8530, 0.0732, 0.6357, 0.8428, 0.1860, 0.4644, 0.6997, 0.3435, 0.9844, 0.9214, 0.7007, 0.5166, 0.7495, 0.7979, 0.9326, 0.9141, 0.9204, 0.1364, 0.4331, 0.9775, 0.3560, 0.7080, 0.4333, 0.1207, 0.7441, 0.4521, 0.6699, 0.0455, 0.9888, 0.1549, 0.6821, 0.2363, 0.2255, 0.2878, 0.0171, 0.0294, 0.8906, 0.5757, 0.0930, 0.4026, 0.2463, 0.6128, 0.3396, 0.5391, 0.5371, 0.7383, 0.6685, 0.3223, 0.9419, 0.7715, 0.0187, 0.0527, 0.8628, 0.1620, 0.2383, 0.6860, 0.6519, 0.5859, 0.6050, 0.2231, 0.8760, ]\n",
      "[0.1804, 0.0188, 0.7246, 0.9688, 0.8579, 0.6147, 0.8364, 0.0751, 0.7100, 0.4265, 0.3032, 0.3208, 0.2930, 0.3538, 0.5562, 0.9570, 0.6030, 0.6704, 0.0769, 0.4468, 0.5713, 0.3682, 0.6475, 0.5674, 0.8228, 0.7349, 0.6138, 0.7734, 0.0321, 0.8706, 0.6846, 0.3826, 0.8271, 0.0818, 0.0718, 0.2063, 0.3457, 0.9526, 0.8286, 0.8306, 0.9722, 0.0792, 0.6958, 0.4727, 0.4475, 0.4646, 0.1514, 0.1793, 0.9844, 0.7832, 0.8979, 0.7104, 0.0321, 0.1005, 0.5010, 0.8989, 0.1943, 0.4973, 0.4277, 0.5674, 0.9087, 0.5796, 0.5474, 0.0185, 0.5898, 0.9712, 0.7495, 0.5942, 0.0491, 0.6685, 0.9746, 0.1598, 0.2222, 0.5488, 0.4854, 0.7993, 0.0320, 0.6567, 0.6323, 0.0093, 0.6353, 0.1803, 0.6514, 0.2583, 0.6396, 0.8638, 0.9072, 0.5181, 0.3958, 0.3635, 0.6113, 0.7422, 0.3486, 0.5903, 0.1615, 0.7065, 0.2404, 0.3418, 0.1227, 0.5107, 0.0810, 0.9883, 0.9907, 0.1705, 0.0912, 0.1997, 0.9849, 0.6646, 0.5479, 0.4883, 0.3606, 0.8457, 0.1383, 0.0559, 0.4241, 0.3472, 0.1627, 0.3694, 0.0816, 0.1786, 0.0247, 0.9185, 0.9214, 0.8750, 0.2839, 0.9199, 0.9395, 0.7832, 0.8574, 0.9888, 0.9673, 0.3987, 0.3999, 0.2278, 0.1760, 0.8271, 0.2271, 0.2546, 0.8271, 0.2578, 0.0879, 0.4600, 0.0093, 0.2903, 0.0914, 0.9600, 0.1844, 0.0864, 0.8408, 0.8306, 0.2279, 0.7124, 0.4792, 0.2238, 0.6406, 0.9438, 0.9048, 0.2808, 0.7764, 0.8911, 0.5669, 0.0064, 0.7139, 0.5029, 0.4551, 0.4758, 0.9907, 0.7822, 0.3196, 0.0894, 0.9619, 0.7178, 0.5801, 0.8594, 0.6748, 0.0714, 0.4487, 0.3975, 0.2910, 0.1719, 0.4119, 0.2864, 0.6572, 0.4480, 0.7295, 0.6763, 0.9209, 0.3252, 0.2490, 0.5308, 0.9546, 0.9873, 0.2404, 0.6694, 0.1627, 0.3418, 0.9893, 0.0875, 0.9639, 0.6060, 0.9028, 0.7979, 0.0293, 0.6567, 0.8115, 0.1840, 0.7812, 0.6064, 0.9448, 0.7432, 0.9902, 0.8623, 0.1665, 0.6143, 0.2661, 0.5879, 0.9854, 0.6470, 0.7695, 0.4688, 0.9897, 0.4194, 0.4209, 0.9868, 0.8691, 0.2949, 0.7437, 0.8179, 0.8770, 0.6519, 0.6680, 0.4451, 0.3689, 0.9336, 0.3406, 0.2123, 0.4116, 0.6187, 0.6050, 0.4255, 0.1350, 0.5303, 0.5093, 0.6270, 0.0640, 0.9204, 0.6323, 0.3467, 0.6694, 0.7466, 0.6421, 0.4038, 0.0327, 0.6606, 0.0954, 0.8149, ]\n",
      "[0.2522, 0.6890, 0.8350, 0.9761, 0.1683, 0.2334, 0.6265, 0.9438, 0.6367, 0.0661, 0.6445, 0.9395, 0.9272, 0.3035, 0.8335, 0.7227, 0.1301, 0.4136, 0.5825, 0.2947, 0.1958, 0.1664, 0.9219, 0.1960, 0.8384, 0.3140, 0.3206, 0.2031, 0.1858, 0.1836, 0.7163, 0.2532, 0.2661, 0.8374, 0.5684, 0.6855, 0.5249, 0.1172, 0.2362, 0.6147, 0.2820, 0.2610, 0.4958, 0.5195, 0.6157, 0.6421, 0.2327, 0.8257, 0.8574, 0.3142, 0.7964, 0.2302, 0.1267, 0.2693, 0.0891, 0.2224, 0.4744, 0.4927, 0.2220, 0.6333, 0.7749, 0.5063, 0.0279, 0.9761, 0.0619, 0.6035, 0.4922, 0.5361, 0.5005, 0.6909, 0.3174, 0.2141, 0.8281, 0.6553, 0.6509, 0.0327, 0.4934, 0.2473, 0.7573, 0.0128, 0.5806, 0.1890, 0.3105, 0.2815, 0.2275, 0.6235, 0.5493, 0.2964, 0.2520, 0.5073, 0.9780, 0.7520, 0.3889, 0.7646, 0.2382, 0.1416, 0.4153, 0.2598, 0.8989, 0.5376, 0.7056, 0.6465, 0.4670, 0.2539, 0.5781, 0.4663, 0.6543, 0.8750, 0.2593, 0.3342, 0.2075, 0.3718, 0.4736, 0.5601, 0.7852, 0.9883, 0.8042, 0.0793, 0.3677, 0.5806, 0.4290, 0.5029, 0.1527, 0.5454, 0.0295, 0.5610, 0.8647, 0.7749, 0.5298, 0.1571, 0.9795, 0.5010, 0.6582, 0.0587, 0.5908, 0.5161, 0.1682, 0.0168, 0.0784, 0.5728, 0.9585, 0.9844, 0.5483, 0.5713, 0.0363, 0.2581, 0.6216, 0.9639, 0.0848, 0.2253, 0.6152, 0.5322, 0.8179, 0.2264, 0.8564, 0.5132, 0.6719, 0.5024, 0.2271, 0.6450, 0.4929, 0.6431, 0.8481, 0.7773, 0.4707, 0.6377, 0.6631, 0.6846, 0.4861, 0.3081, 0.0539, 0.4475, 0.3618, 0.5845, 0.0224, 0.7407, 0.1057, 0.7515, 0.7715, 0.9541, 0.1033, 0.0607, 0.3079, 0.0958, 0.1152, 0.5952, 0.1108, 0.8281, 0.9722, 0.4006, 0.0484, 0.6143, 0.7871, 0.4749, 0.7217, 0.9414, 0.0671, 0.9619, 0.9458, 0.1669, 0.0282, 0.7153, 0.7437, 0.3237, 0.1323, 0.2615, 0.3879, 0.8784, 0.3689, 0.8086, 0.4124, 0.2942, 0.1538, 0.1588, 0.0554, 0.7266, 0.2817, 0.8838, 0.8467, 0.1931, 0.1225, 0.6646, 0.7554, 0.8828, 0.5376, 0.8154, 0.7173, 0.1630, 0.8403, 0.1332, 0.3130, 0.7266, 0.2262, 0.0735, 0.8711, 0.4519, 0.5801, 0.7812, 0.6704, 0.6787, 0.6562, 0.0252, 0.0170, 0.4868, 0.1122, 0.5117, 0.2291, 0.5498, 0.5308, 0.4822, 0.4946, 0.9492, 0.4922, 0.0814, 0.5371, 0.9219, ]\n",
      "[0.0682, 0.3601, 0.7417, 0.7998, 0.8682, 0.6162, 0.6694, 0.3533, 0.1639, 0.3835, 0.3381, 0.7266, 0.7690, 0.4788, 0.3328, 0.1467, 0.8599, 0.3777, 0.4680, 0.6812, 0.9448, 0.7915, 0.1890, 0.7056, 0.3745, 0.1753, 0.6523, 0.8667, 0.7373, 0.7090, 0.7783, 0.0254, 0.4902, 0.0693, 0.8979, 0.9077, 0.5757, 0.0433, 0.0349, 0.5171, 0.0963, 0.4250, 0.9160, 0.5620, 0.5205, 0.7158, 0.5435, 0.1428, 0.1991, 0.0912, 0.6782, 0.3953, 0.3342, 0.0057, 0.8857, 0.0030, 0.8774, 0.9346, 0.9131, 0.7646, 0.1876, 0.4492, 0.5693, 0.6406, 0.5405, 0.2289, 0.0731, 0.5386, 0.6362, 0.8926, 0.4497, 0.3521, 0.7334, 0.2122, 0.4624, 0.4202, 0.3638, 0.8613, 0.7295, 0.2168, 0.9639, 0.9854, 0.5576, 0.6538, 0.3445, 0.3889, 0.3794, 0.3967, 0.6562, 0.1254, 0.8643, 0.4084, 0.8916, 0.3386, 0.3579, 0.1664, 0.5830, 0.6187, 0.8423, 0.7104, 0.9312, 0.6348, 0.6934, 0.8799, 0.6904, 0.5742, 0.8701, 0.5986, 0.5591, 0.8198, 0.8779, 0.5537, 0.8198, 0.3008, 0.2422, 0.4434, 0.8125, 0.3806, 0.8545, 0.7998, 0.5640, 0.6201, 0.1791, 0.4365, 0.2917, 0.1416, 0.7480, 0.6553, 0.0591, 0.7856, 0.5825, 0.8843, 0.3018, 0.6655, 0.6953, 0.2576, 0.6162, 0.2242, 0.6548, 0.0114, 0.2786, 0.7368, 0.8311, 0.6768, 0.1963, 0.4543, 0.6699, 0.5767, 0.7036, 0.7476, 0.8003, 0.2583, 0.8086, 0.1068, 0.8892, 0.5815, 0.9497, 0.7017, 0.7036, 0.7900, 0.8574, 0.9873, 0.7202, 0.6680, 0.7031, 0.7490, 0.0648, 0.9814, 0.6680, 0.9756, 0.3904, 0.8525, 0.3953, 0.5415, 0.8921, 0.8101, 0.8477, 0.1302, 0.6562, 0.4600, 0.5991, 0.0032, 0.8872, 0.0784, 0.3213, 0.8076, 0.5210, 0.4468, 0.1866, 0.0305, 0.8589, 0.2549, 0.7944, 0.8027, 0.8135, 0.4678, 0.1895, 0.0090, 0.4246, 0.8281, 0.8936, 0.4177, 0.7275, 0.3743, 0.4854, 0.0364, 0.9595, 0.9214, 0.7217, 0.4858, 0.0901, 0.2527, 0.9512, 0.2639, 0.5640, 0.9277, 0.7134, 0.9727, 0.9297, 0.1360, 0.0267, 0.3652, 0.3665, 0.0611, 0.2717, 0.5698, 0.1041, 0.1486, 0.0533, 0.8750, 0.9673, 0.5898, 0.3201, 0.7412, 0.9370, 0.9048, 0.1193, 0.8286, 0.7021, 0.9272, 0.4326, 0.6641, 0.4143, 0.4905, 0.9771, 0.1472, 0.8984, 0.5708, 0.8643, 0.0203, 0.2385, 0.3240, 0.1462, 0.0845, 0.5410, 0.5132, ]\n",
      "[0.6621, 0.4832, 0.5425, 0.2496, 0.6846, 0.6299, 0.8799, 0.9556, 0.0033, 0.9668, 0.0648, 0.5498, 0.8398, 0.6323, 0.2607, 0.4600, 0.2893, 0.9180, 0.4895, 0.6133, 0.5693, 0.7495, 0.4729, 0.0490, 0.0857, 0.7715, 0.4285, 0.8691, 0.6470, 0.0491, 0.1626, 0.1144, 0.8125, 0.9863, 0.2231, 0.6426, 0.6665, 0.3445, 0.0293, 0.7593, 0.3723, 0.5894, 0.6982, 0.9077, 0.8540, 0.7402, 0.4746, 0.1809, 0.0123, 0.8223, 0.3611, 0.8560, 0.2042, 0.5298, 0.6396, 0.4971, 0.3210, 0.5527, 0.8491, 0.1603, 0.1342, 0.6431, 0.5669, 0.3447, 0.5171, 0.9785, 0.3147, 0.4778, 0.1254, 0.6572, 0.4966, 0.5742, 0.9980, 0.5259, 0.1678, 0.0961, 0.4998, 0.0488, 0.9878, 0.4119, 0.5967, 0.1099, 0.0376, 0.4172, 0.6206, 0.5088, 0.1642, 0.6333, 0.7402, 0.0446, 0.0378, 0.4043, 0.4934, 0.4751, 0.4067, 0.1180, 0.2949, 0.7827, 0.3965, 0.3584, 0.8955, 0.6367, 0.8940, 0.7227, 0.3765, 0.2244, 0.6899, 0.9341, 0.7529, 0.0876, 0.2844, 0.6377, 0.1337, 0.2393, 0.6846, 0.1471, 0.4426, 0.3308, 0.7178, 0.1572, 0.6294, 0.1002, 0.5327, 0.3467, 0.8398, 0.0698, 0.0590, 0.8320, 0.6470, 0.0304, 0.5176, 0.8994, 0.9434, 0.0854, 0.8306, 0.8799, 0.9824, 0.7451, 0.2964, 0.1444, 0.1561, 0.0699, 0.7710, 0.1056, 0.8560, 0.2006, 0.4287, 0.8564, 0.8984, 0.4529, 0.9873, 0.0845, 0.6562, 0.4514, 0.9722, 0.6025, 0.1058, 0.7256, 0.7241, 0.2966, 0.7998, 0.1726, 0.2246, 0.6797, 0.5283, 0.8174, 0.0064, 0.2419, 0.9258, 0.9199, 0.5601, 0.3730, 0.9097, 0.6938, 0.6768, 0.1104, 0.1370, 0.6836, 0.0196, 0.4175, 0.7578, 0.1077, 0.4883, 0.8726, 0.7710, 0.0387, 0.4788, 0.3660, 0.1730, 0.4106, 0.7075, 0.2434, 0.8359, 0.9863, 0.6006, 0.1262, 0.3215, 0.8687, 0.9580, 0.1155, 0.4675, 0.8457, 0.2751, 0.1992, 0.8330, 0.2695, 0.9419, 0.3264, 0.0731, 0.6318, 0.6567, 0.1660, 0.2617, 0.8770, 0.3464, 0.6191, 0.8149, 0.2017, 0.7314, 0.7915, 0.1637, 0.7007, 0.7046, 0.5562, 0.3774, 0.4961, 0.5308, 0.2251, 0.1351, 0.2546, 0.9473, 0.9268, 0.0872, 0.5015, 0.2742, 0.9331, 0.8208, 0.2659, 0.9355, 0.2783, 0.7773, 0.3103, 0.9209, 0.3062, 0.1421, 0.9585, 0.5122, 0.3325, 0.2512, 0.2488, 0.6851, 0.8101, 0.6045, 0.5503, 0.3269, 0.4260, ]]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(    0.0000, device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.isclose(dumb_attn(Q, K, V), flashattn(Q, K, V))\n",
    "# dumb_attn(Q, K, V), flashattn(Q, K, V)\n",
    "\n",
    "# F.scaled_dot_product_attention(Q, K, V, scale=1), flashattn(Q, K, V)\n",
    "# module.flash_attn(Q, K, V)\n",
    "torch.set_printoptions(profile='full', sci_mode=False)\n",
    "r1 = dumb_attn(Q, K, V)\n",
    "r2 = module.flash_attn(Q, K, V)\n",
    "(r1-r2).abs().mean()\n",
    "# r1, r2\n",
    "\n",
    "\n",
    "# # r1, dumb_attn(Q.float(), K.float(), V.float()), r2\n",
    "# # r1, r2[:50]\n",
    "# dumb_attn(Q, K, V), module.flash_attn(Q, K, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tc=12, tr=6\n",
      "Launching kernel with smem size 1600\n",
      "Difference on (48, 32) tensors: 0.00038886070251464844\n",
      "\n",
      "\n",
      "tc=16, tr=8\n",
      "Launching kernel with smem size 12352\n",
      "Difference on (64, 256) tensors: 0.001415252685546875\n",
      "\n",
      "\n",
      "tc=256, tr=128\n",
      "Launching kernel with smem size 12352\n",
      "Difference on (1024, 256) tensors: 0.002254486083984375\n",
      "\n",
      "\n",
      "tc=1024, tr=512\n",
      "Launching kernel with smem size 49216\n",
      "Difference on (4096, 1024) tensors: 0.00667572021484375\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def test_flashattn():\n",
    "    diff = lambda r1, r2: (r1-r2).abs().mean()\n",
    "    dims = ((48, 32), (64, 256), (1024, 256), (4096, 1024))\n",
    "    for N, d in dims:\n",
    "        Q = torch.rand((N, d), dtype=torch.float16, device='cuda') \n",
    "        K = torch.rand((N, d), dtype=torch.float16, device='cuda') \n",
    "        V = torch.rand((N, d), dtype=torch.float16, device='cuda') \n",
    "\n",
    "        # print(f'Difference on ({N}, {d}) tensors: {diff(dumb_attn(Q, K, V), F.scaled_dot_product_attention(Q, K, V, scale=1))}\\n\\n')\n",
    "        print(f'Difference on ({N}, {d}) tensors: {diff(module.flash_attn(Q, K, V), F.scaled_dot_product_attention(Q, K, V, scale=1))}\\n\\n')\n",
    "        # print(f'Difference on ({N}, {d}) tensors: {diff(module.flash_attn(Q, K, V), dumb_attn(Q, K, V))}\\n\\n')\n",
    "\n",
    "torch.set_printoptions(profile='short', sci_mode=False)\n",
    "test_flashattn()\n",
    "torch.set_printoptions(profile='default', sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cudaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
