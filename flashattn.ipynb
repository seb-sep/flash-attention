{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure to use the right version of pytorch when testing on nvidia/amd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "torch.set_printoptions(sci_mode=False)\n",
    "# from torch.nn.functional import scaled_dot_product_attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "M = 32\n",
    "\n",
    "\n",
    "def flashattn(Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor) -> torch.Tensor:\n",
    "    assert Q.shape == K.shape == V.shape\n",
    "    assert len(Q.shape) == 2\n",
    "\n",
    "    K = K.T\n",
    "\n",
    "    # seq length, inner dim of representations\n",
    "    N, d = Q.shape\n",
    "\n",
    "    # of kv vectors per tile\n",
    "    bc = math.ceil(M/(4*d))\n",
    "    tc = math.ceil(N/bc)\n",
    "    K_shared = torch.empty((d, bc), dtype=Q.dtype)\n",
    "    V_shared = torch.empty((bc, d), dtype=Q.dtype)\n",
    "\n",
    "    # of q vectors per tile\n",
    "    br = min(math.ceil(M/(4*d)), d)\n",
    "    tr = math.ceil(N/br)\n",
    "    Q_shared = torch.empty((br, d), dtype=Q.dtype)\n",
    "\n",
    "    # print(f'bc={bc}, br={br}')\n",
    "    # print(f'tc={tc}, tr={tr}')\n",
    "\n",
    "    # output matrix\n",
    "    O = torch.zeros_like(Q)\n",
    "    O_shared = torch.empty_like(Q_shared)\n",
    "\n",
    "    # intermediate rowmaxes\n",
    "    m = torch.full((N,), -torch.inf, dtype=Q.dtype)\n",
    "    m_shared = torch.empty((br, 1), dtype=Q.dtype)\n",
    "    # intermediate normalization constants\n",
    "    l = torch.full((N,), 0, dtype=Q.dtype)\n",
    "    l_shared = torch.empty((br, 1), dtype=Q.dtype)\n",
    "\n",
    "    for i in range(tc):\n",
    "        # load k, v chunks\n",
    "        # make sure we load in k as its transposed version\n",
    "        K_shared[:, :] = K[:, i*bc:(i+1)*bc]\n",
    "        V_shared[:, :] = V[i*bc:(i+1)*bc, :]\n",
    "\n",
    "        for j in range(tr):\n",
    "            # load in q, o, m, l\n",
    "            Q_shared[:, :] = Q[j*br:(j+1)*br, :]\n",
    "            # if i == 0: print(f'Q: {Q_shared}')\n",
    "            O_shared[:, :] = O[j*br:(j+1)*br, :]\n",
    "            m_shared[:, :] = m[j*br:(j+1)*br].unsqueeze(-1)\n",
    "            l_shared[:, :] = l[j*br:(j+1)*br].unsqueeze(-1)\n",
    "            \n",
    "            S = Q_shared @ K_shared\n",
    "\n",
    "            # get row-wise softmax statistics\n",
    "            mt = S.max(dim=1).values.reshape(-1, 1)\n",
    "\n",
    "            Pt = torch.exp(S - mt)\n",
    "            lt = Pt.sum(dim=1).reshape(-1, 1)\n",
    "\n",
    "            # compute new statistics\n",
    "            m_new = torch.max(mt, m_shared)\n",
    "            l_new = (torch.exp(m_shared - m_new) * l_shared) + (torch.exp(mt - m_new) * lt)\n",
    "\n",
    "\n",
    "            # update chunk of output\n",
    "            O_new = (l_shared * torch.exp(m_shared - m_new) * O_shared + torch.exp(mt - m_new) * Pt @ V_shared) / l_new\n",
    "            O[j*br:(j+1)*br, :] = O_new\n",
    "            \n",
    "            m[j*br:(j+1)*br] = m_new.flatten()\n",
    "            l[j*br:(j+1)*br] = l_new.flatten()\n",
    "\n",
    "    return O\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.3616, 0.2683, 0.4976,  ..., 0.7031, 0.2281, 0.2144],\n",
       "         [0.1565, 0.8208, 0.2571,  ..., 0.4819, 0.5317, 0.2581],\n",
       "         [0.1309, 0.3518, 0.7847,  ..., 0.0644, 0.9189, 0.5454],\n",
       "         ...,\n",
       "         [0.3674, 0.7979, 0.2603,  ..., 0.3276, 0.1978, 0.4421],\n",
       "         [0.1500, 0.5010, 0.4197,  ..., 0.9150, 0.4585, 0.0547],\n",
       "         [0.2473, 0.0584, 0.1126,  ..., 0.7305, 0.5952, 0.0113]],\n",
       "        device='cuda:0', dtype=torch.float16),\n",
       " tensor([[0.0080, 0.0992, 0.6978,  ..., 0.2108, 0.5122, 0.1340],\n",
       "         [0.7764, 0.0851, 0.0626,  ..., 0.6558, 0.2286, 0.3462],\n",
       "         [0.7935, 0.3059, 0.4109,  ..., 0.0846, 0.9390, 0.9888],\n",
       "         ...,\n",
       "         [0.8149, 0.5093, 0.8228,  ..., 0.4124, 0.8721, 0.9038],\n",
       "         [0.0586, 0.8975, 0.9458,  ..., 0.7231, 0.3137, 0.9795],\n",
       "         [0.4402, 0.0186, 0.9463,  ..., 0.5972, 0.9468, 0.5835]],\n",
       "        device='cuda:0', dtype=torch.float16),\n",
       " tensor([[0.0867, 0.5601, 0.5176,  ..., 0.6035, 0.5410, 0.1606],\n",
       "         [0.6162, 0.7456, 0.4070,  ..., 0.5420, 0.2908, 0.3499],\n",
       "         [0.1104, 0.6519, 0.5908,  ..., 0.8496, 0.2654, 0.3713],\n",
       "         ...,\n",
       "         [0.9829, 0.3430, 0.7627,  ..., 0.3711, 0.2908, 0.2078],\n",
       "         [0.7910, 0.9355, 0.3044,  ..., 0.5596, 0.1089, 0.0607],\n",
       "         [0.2015, 0.8301, 0.6646,  ..., 0.9004, 0.8647, 0.2382]],\n",
       "        device='cuda:0', dtype=torch.float16))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import sqrt\n",
    "\n",
    "def dumb_attn(Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"equivalent to F.scaled_dot_product_attention(Q, K, V, scale=1)\"\"\"\n",
    "    d = Q.shape[1]\n",
    "    return torch.softmax((Q)@ K.T, dim=1) @ V\n",
    "    # return F.scaled_dot_product_attention(Q, K, V, scale=1)\n",
    "\n",
    "\n",
    "# have to support up to like d=128\n",
    "# Model dim of 4096 is divided ACROSS HEADS\n",
    "# we're giving up and only supporting fp16 (like flashattn)\n",
    "# Q = torch.rand((8196, 128), dtype=torch.float16, device='cuda') \n",
    "# K = torch.rand((8196, 128), dtype=torch.float16, device='cuda') \n",
    "# V = torch.rand((8196, 128), dtype=torch.float16, device='cuda') \n",
    "\n",
    "# K[-1] = torch.zeros((2,))\n",
    "# V[-1] = torch.zeros((2,))\n",
    "\n",
    "\n",
    "# Q = torch.randint(1, 9, (32, 16), device='cuda').to(torch.float16)\n",
    "# K = torch.randint(1, 9, (32, 16), device='cuda').to(torch.float16)\n",
    "# V = torch.randint(1, 9, (32, 16), device='cuda').to(torch.float16)\n",
    "\n",
    "Q = torch.rand((64, 128), device='cuda').to(torch.float16)\n",
    "K = torch.rand((64, 128), device='cuda').to(torch.float16)\n",
    "V = torch.rand((64, 128), device='cuda').to(torch.float16)\n",
    "\n",
    "# Q = torch.tensor([[0., 1.],\n",
    "#          [2., 3.]])\n",
    "\n",
    "# K = torch.tensor([[0., 1.5],\n",
    "#          [2., 3.]])\n",
    "\n",
    "# V = torch.tensor([[1., 0.],\n",
    "#          [0., 1.]])\n",
    "Q, K, V\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/seb/Code/flash-attention/rocm/flash_attention.hip -> /home/seb/Code/flash-attention/rocm/flash_attention_hip.hip [ok]\n",
      "Total number of unsupported CUDA function calls: 0\n",
      "\n",
      "\n",
      "Total number of replaced kernel launches: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The input conditions for extension module m have changed. Bumping to version 7 and re-building as m_v7...\n",
      "\u001b[92mSuccessfully preprocessed all matching files.\u001b[0m\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file build/build.ninja...\n",
      "Building extension module m_v7...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/2] /opt/rocm-6.1.3/bin/hipcc  -DWITH_HIP -DTORCH_EXTENSION_NAME=m_v7 -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/seb/Code/pyenvs/rocmenv/lib/python3.12/site-packages/torch/include -isystem /home/seb/Code/pyenvs/rocmenv/lib/python3.12/site-packages/torch/include/torch/csrc/api/include -isystem /home/seb/Code/pyenvs/rocmenv/lib/python3.12/site-packages/torch/include/TH -isystem /home/seb/Code/pyenvs/rocmenv/lib/python3.12/site-packages/torch/include/THC -isystem /home/seb/Code/pyenvs/rocmenv/lib/python3.12/site-packages/torch/include/THH -isystem /opt/rocm-6.1.3/include -isystem /home/seb/miniconda3/include/python3.12 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 --offload-arch=\"gfx1100\" -fPIC -D__HIP_PLATFORM_AMD__=1 -DUSE_ROCM=1 -DHIPBLAS_V2 -DCUDA_HAS_FP16=1 -D__HIP_NO_HALF_OPERATORS__=1 -D__HIP_NO_HALF_CONVERSIONS__=1 -fno-gpu-rdc -c /home/seb/Code/flash-attention/rocm/flash_attention_hip.hip -o flash_attention_hip.cuda.o \n",
      "/home/seb/Code/flash-attention/rocm/flash_attention_hip.hip:555:5: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  555 |     hipGetDeviceProperties(&props, 0);\n",
      "      |     ^~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~~\n",
      "/opt/rocm-6.1.3/include/hip/hip_runtime_api.h:91:32: note: expanded from macro 'hipGetDeviceProperties'\n",
      "   91 | #define hipGetDeviceProperties hipGetDevicePropertiesR0600\n",
      "      |                                ^~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/home/seb/Code/flash-attention/rocm/flash_attention_hip.hip:657:5: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  657 |     hipEventCreate(&start);\n",
      "      |     ^~~~~~~~~~~~~~ ~~~~~~\n",
      "/home/seb/Code/flash-attention/rocm/flash_attention_hip.hip:658:5: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  658 |     hipEventCreate(&stop);\n",
      "      |     ^~~~~~~~~~~~~~ ~~~~~\n",
      "/home/seb/Code/flash-attention/rocm/flash_attention_hip.hip:659:5: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  659 |     hipEventRecord(start);\n",
      "      |     ^~~~~~~~~~~~~~\n",
      "/home/seb/Code/flash-attention/rocm/flash_attention_hip.hip:665:5: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  665 |     hipEventRecord(stop);\n",
      "      |     ^~~~~~~~~~~~~~\n",
      "/home/seb/Code/flash-attention/rocm/flash_attention_hip.hip:666:5: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  666 |     hipEventSynchronize(stop);\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~\n",
      "/home/seb/Code/flash-attention/rocm/flash_attention_hip.hip:668:5: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  668 |     hipEventElapsedTime(&ms, start, stop);\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~\n",
      "/home/seb/Code/flash-attention/rocm/flash_attention_hip.hip:671:5: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  671 |     hipEventDestroy(start);\n",
      "      |     ^~~~~~~~~~~~~~~ ~~~~~\n",
      "/home/seb/Code/flash-attention/rocm/flash_attention_hip.hip:672:5: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  672 |     hipEventDestroy(stop);\n",
      "      |     ^~~~~~~~~~~~~~~ ~~~~\n",
      "9 warnings generated when compiling for gfx1100.\n",
      "/home/seb/Code/flash-attention/rocm/flash_attention_hip.hip:555:5: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  555 |     hipGetDeviceProperties(&props, 0);\n",
      "      |     ^~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~~\n",
      "/opt/rocm-6.1.3/include/hip/hip_runtime_api.h:91:32: note: expanded from macro 'hipGetDeviceProperties'\n",
      "   91 | #define hipGetDeviceProperties hipGetDevicePropertiesR0600\n",
      "      |                                ^~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
      "/home/seb/Code/flash-attention/rocm/flash_attention_hip.hip:657:5: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  657 |     hipEventCreate(&start);\n",
      "      |     ^~~~~~~~~~~~~~ ~~~~~~\n",
      "/home/seb/Code/flash-attention/rocm/flash_attention_hip.hip:658:5: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  658 |     hipEventCreate(&stop);\n",
      "      |     ^~~~~~~~~~~~~~ ~~~~~\n",
      "/home/seb/Code/flash-attention/rocm/flash_attention_hip.hip:659:5: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  659 |     hipEventRecord(start);\n",
      "      |     ^~~~~~~~~~~~~~\n",
      "/home/seb/Code/flash-attention/rocm/flash_attention_hip.hip:665:5: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  665 |     hipEventRecord(stop);\n",
      "      |     ^~~~~~~~~~~~~~\n",
      "/home/seb/Code/flash-attention/rocm/flash_attention_hip.hip:666:5: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  666 |     hipEventSynchronize(stop);\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~\n",
      "/home/seb/Code/flash-attention/rocm/flash_attention_hip.hip:668:5: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  668 |     hipEventElapsedTime(&ms, start, stop);\n",
      "      |     ^~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~\n",
      "/home/seb/Code/flash-attention/rocm/flash_attention_hip.hip:671:5: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  671 |     hipEventDestroy(start);\n",
      "      |     ^~~~~~~~~~~~~~~ ~~~~~\n",
      "/home/seb/Code/flash-attention/rocm/flash_attention_hip.hip:672:5: warning: ignoring return value of function declared with 'nodiscard' attribute [-Wunused-result]\n",
      "  672 |     hipEventDestroy(stop);\n",
      "      |     ^~~~~~~~~~~~~~~ ~~~~\n",
      "9 warnings generated when compiling for host.\n",
      "[2/2] c++ flash_attention_hip.cuda.o -shared -L/home/seb/Code/pyenvs/rocmenv/lib/python3.12/site-packages/torch/lib -lc10 -lc10_hip -ltorch_cpu -ltorch_hip -ltorch -ltorch_python -L/opt/rocm-6.1.3/lib -lamdhip64 -o m_v7.so\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module m_v7...\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.cpp_extension import load\n",
    "module = load(\n",
    "    name='m',\n",
    "    sources=['rocm/flash_attention.hip',],\n",
    "    extra_cflags=['--offload-arch=\"gfx1100\"'],\n",
    "    build_directory='build',\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0002, device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.isclose(dumb_attn(Q, K, V), flashattn(Q, K, V))\n",
    "# dumb_attn(Q, K, V), flashattn(Q, K, V)\n",
    "\n",
    "# F.scaled_dot_product_attention(Q, K, V, scale=1), flashattn(Q, K, V)\n",
    "# module.flash_attn(Q, K, V)\n",
    "torch.set_printoptions(profile='default', sci_mode=False)\n",
    "r1 = dumb_attn(Q, K, V)\n",
    "r2 = module.flash_attn(Q, K, V)\n",
    "(r1-r2).abs().mean()\n",
    "# r1, r2\n",
    "# Q @ K.T\n",
    "\n",
    "\n",
    "# # r1, dumb_attn(Q.float(), K.float(), V.float()), r2\n",
    "# # r1, r2[:50]\n",
    "# dumb_attn(Q, K, V), module.flash_attn(Q, K, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference on (48, 32) tensors: 0.0002092123031616211\n",
      "\n",
      "\n",
      "Difference on (64, 128) tensors: 0.0005679130554199219\n",
      "\n",
      "\n",
      "Difference on (1024, 128) tensors: 0.0006666183471679688\n",
      "\n",
      "\n",
      "Difference on (4096, 128) tensors: 0.0012874603271484375\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def test_flashattn():\n",
    "    diff = lambda r1, r2: (r1-r2).abs().mean()\n",
    "    dims = ((48, 32), (64, 128), (1024, 128), (4096, 128))\n",
    "    for N, d in dims:\n",
    "        Q = torch.rand((N, d), dtype=torch.float16, device='cuda') \n",
    "        K = torch.rand((N, d), dtype=torch.float16, device='cuda') \n",
    "        V = torch.rand((N, d), dtype=torch.float16, device='cuda') \n",
    "\n",
    "        # print(f'Difference on ({N}, {d}) tensors: {diff(dumb_attn(Q, K, V), F.scaled_dot_product_attention(Q, K, V, scale=1))}\\n\\n')\n",
    "        res = module.flash_attn(Q, K, V)\n",
    "        # print(torch.isnan(res).any())\n",
    "        # print(res, res.shape)\n",
    "        print(f'Difference on ({N}, {d}) tensors: {diff(res, F.scaled_dot_product_attention(Q, K, V, scale=1))}\\n\\n')\n",
    "\n",
    "        # print(f'Difference on ({N}, {d}) tensors: {diff(module.flash_attn(Q, K, V), dumb_attn(Q, K, V))}\\n\\n')\n",
    "\n",
    "torch.set_printoptions(profile='full', sci_mode=False)\n",
    "test_flashattn()\n",
    "torch.set_printoptions(profile='default', sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 0., 0., 1., 1., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1.],\n",
       "         [0., 0., 0., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0.],\n",
       "         [0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0.],\n",
       "         [1., 1., 0., 1., 1., 1., 0., 0., 0., 1., 1., 1., 0., 0., 1., 1.],\n",
       "         [0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 1., 1., 1.],\n",
       "         [1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1.],\n",
       "         [1., 1., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 0., 1., 1., 0.],\n",
       "         [1., 0., 0., 0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 1., 0.],\n",
       "         [0., 0., 0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1., 0.],\n",
       "         [0., 0., 0., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0., 1., 1., 0.],\n",
       "         [0., 0., 0., 1., 1., 1., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1.],\n",
       "         [1., 0., 0., 1., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1.],\n",
       "         [1., 1., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0.],\n",
       "         [1., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 1.],\n",
       "         [0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0.],\n",
       "         [0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0.]],\n",
       "        device='cuda:0', dtype=torch.float16),\n",
       " tensor([[1., 0., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
       "          0., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1.],\n",
       "         [1., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 1.,\n",
       "          1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0., 1.],\n",
       "         [1., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1.,\n",
       "          1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0., 1.],\n",
       "         [0., 1., 0., 0., 1., 1., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 0., 1.,\n",
       "          0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 0., 0., 0., 0.],\n",
       "         [0., 1., 1., 1., 0., 0., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0.,\n",
       "          0., 1., 0., 0., 0., 1., 1., 1., 0., 1., 0., 0., 0., 0.],\n",
       "         [0., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0., 1., 1., 1., 1., 0., 1., 1.,\n",
       "          1., 0., 0., 1., 0., 0., 0., 1., 1., 0., 1., 0., 0., 0.],\n",
       "         [1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0.,\n",
       "          0., 0., 1., 0., 1., 1., 1., 0., 1., 1., 0., 0., 1., 1.],\n",
       "         [0., 0., 1., 0., 1., 0., 1., 0., 1., 1., 0., 1., 0., 1., 1., 1., 0., 0.,\n",
       "          0., 0., 1., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1.],\n",
       "         [1., 0., 0., 1., 0., 0., 0., 0., 1., 1., 0., 0., 1., 0., 0., 1., 0., 1.,\n",
       "          1., 0., 0., 1., 1., 0., 0., 0., 0., 1., 1., 0., 1., 0.],\n",
       "         [1., 1., 1., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
       "          0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0.],\n",
       "         [0., 0., 1., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 1., 1., 0.,\n",
       "          1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 1., 1., 1., 0.],\n",
       "         [0., 0., 1., 1., 1., 0., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
       "          0., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 1.],\n",
       "         [1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0.,\n",
       "          1., 1., 1., 1., 1., 1., 1., 1., 0., 1., 0., 0., 0., 0.],\n",
       "         [0., 0., 0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1.,\n",
       "          0., 1., 1., 1., 1., 0., 0., 1., 0., 1., 1., 0., 1., 0.],\n",
       "         [0., 1., 0., 1., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 0., 1., 1.,\n",
       "          0., 0., 0., 1., 1., 1., 0., 1., 1., 1., 1., 1., 0., 0.],\n",
       "         [1., 0., 1., 0., 0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 0.,\n",
       "          0., 1., 0., 1., 1., 1., 0., 1., 1., 1., 1., 0., 1., 0.]],\n",
       "        device='cuda:0', dtype=torch.float16))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A = torch.rand((16, 32), dtype=torch.float16, device='cuda')\n",
    "# B = torch.rand((32, 16), dtype=torch.float16, device='cuda')\n",
    "\n",
    "A = torch.randint(0, 2, (16, 16), dtype=torch.float16, device='cuda')\n",
    "B = torch.randint(0, 2, (16, 32), dtype=torch.float16, device='cuda')\n",
    "\n",
    "# A = torch.eye(16, dtype=torch.float16, device='cuda') \n",
    "# B = torch.eye(16, dtype=torch.float16, device='cuda')\n",
    "\n",
    "# A = torch.ones((16, 16), dtype=torch.float16, device='cuda') \n",
    "# B = torch.zeros((16, 16), dtype=torch.float16, device='cuda') \n",
    "\n",
    "A, B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warp 0 computing tile 0 of matrix, coords (0, 0), will do 1 loops\n",
      "Warp 0 loading from idx 0 of a and idx 0 of b\n",
      "Warp 0 computing tile 1 of matrix, coords (0, 1), will do 1 loops\n",
      "Warp 0 loading from idx 0 of a and idx 16 of b\n",
      "True\n",
      "Warp 0 computing tile 0 of matrix, coords (0, 0), will do 1 loops\n",
      "Warp 0 loading from idx 0 of a and idx 0 of b\n",
      "Warp 0 computing tile 1 of matrix, coords (0, 1), will do 1 loops\n",
      "Warp 0 loading from idx 0 of a and idx 16 of b\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0',\n",
       "       dtype=torch.float16)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print(module.matmul(A, B), (A @ B))\n",
    "torch.set_printoptions(profile='full', sci_mode=False)\n",
    "print(torch.allclose(module.matmul(A, B, False), A @ B))\n",
    "module.matmul(A, B, False) - A @ B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 16]) torch.Size([16, 48])\n",
      "Warp 0 computing tile 0 of matrix, coords (0, 0), will do 1 loops\n",
      "Warp 0 loading from idx 0 of a and idx 0 of b\n",
      "Warp 0 computing tile 1 of matrix, coords (0, 1), will do 1 loops\n",
      "Warp 0 loading from idx 0 of a and idx 16 of b\n",
      "Warp 0 computing tile 2 of matrix, coords (0, 2), will do 1 loops\n",
      "Warp 0 loading from idx 0 of a and idx 32 of b\n",
      "Warp 0 computing tile 3 of matrix, coords (1, 0), will do 1 loops\n",
      "Warp 0 loading from idx 256 of a and idx 0 of b\n",
      "Warp 0 computing tile 4 of matrix, coords (1, 1), will do 1 loops\n",
      "Warp 0 loading from idx 256 of a and idx 16 of b\n",
      "Warp 0 computing tile 5 of matrix, coords (1, 2), will do 1 loops\n",
      "Warp 0 loading from idx 256 of a and idx 32 of b\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[138.,  45.,  47., 115.,  83.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [196.,  90.,  74., 156., 118.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [129.,  66.,  45., 106., 101.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [176., 105.,  81., 149., 123.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [181.,  98.,  75., 141., 106.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.]],\n",
       "        device='cuda:0', dtype=torch.float16),\n",
       " tensor([[138.,  45.,  47., 115.,  83.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [196.,  90.,  74., 156., 118.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [129.,  66.,  45., 106., 101.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [176., 105.,  81., 149., 123.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [181.,  98.,  75., 141., 106.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
       "            0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.]],\n",
       "        device='cuda:0', dtype=torch.float16))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Qp = F.pad(Q, (0, 16-4, 0, 32-5))\n",
    "Kp = F.pad(K, (0, 16-4, 0, 48-5))\n",
    "\n",
    "# Qp, Kp\n",
    "print(Qp.shape, Kp.T.shape)\n",
    "(module.matmul(Qp, Kp.T.contiguous(), False), (Qp @ Kp.T))\n",
    "# torch.allclose(module.matmul(Qp, Kp.T, False), module.matmul(Qp, Kp, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warp 0 computing tile 0 of matrix, coords (0, 0), will do 1 loops\n",
      "Warp 0 loading from idx 0 of a and idx 0 of b\n",
      "Warp 0 computing tile 1 of matrix, coords (0, 1), will do 1 loops\n",
      "Warp 0 loading from idx 0 of a and idx 16 of b\n",
      "Warp 0 computing tile 2 of matrix, coords (0, 2), will do 1 loops\n",
      "Warp 0 loading from idx 0 of a and idx 32 of b\n",
      "Warp 0 computing tile 3 of matrix, coords (1, 0), will do 1 loops\n",
      "Warp 0 loading from idx 256 of a and idx 0 of b\n",
      "Warp 0 computing tile 4 of matrix, coords (1, 1), will do 1 loops\n",
      "Warp 0 loading from idx 256 of a and idx 16 of b\n",
      "Warp 0 computing tile 5 of matrix, coords (1, 2), will do 1 loops\n",
      "Warp 0 loading from idx 256 of a and idx 32 of b\n",
      "Warp 0 computing tile 0 of matrix, coords (0, 0), will do 1 loops\n",
      "Warp 0 loading from idx 0 of a and idx 0 of b\n",
      "Warp 0 computing tile 1 of matrix, coords (0, 1), will do 1 loops\n",
      "Warp 0 loading from idx 0 of a and idx 16 of b\n",
      "Warp 0 computing tile 2 of matrix, coords (0, 2), will do 1 loops\n",
      "Warp 0 loading from idx 0 of a and idx 32 of b\n",
      "Warp 0 computing tile 3 of matrix, coords (1, 0), will do 1 loops\n",
      "Warp 0 loading from idx 256 of a and idx 0 of b\n",
      "Warp 0 computing tile 4 of matrix, coords (1, 1), will do 1 loops\n",
      "Warp 0 loading from idx 256 of a and idx 16 of b\n",
      "Warp 0 computing tile 5 of matrix, coords (1, 2), will do 1 loops\n",
      "Warp 0 loading from idx 256 of a and idx 32 of b\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(module.matmul(Qp, Kp.T.contiguous(), False), module.matmul(Qp, Kp, True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32768"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# module.matmul(B, Kp, False), module.matmul(B, Kp.T.contiguous(), False)\n",
    "8192*4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cudaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
